# ============================================
# CONFIGURACIÓN DE MODELOS ML - REFACTORIZADA
# ============================================

models:
  
  # Embeddings para diferentes tipos de contenido
  embeddings:
    text:
      default: "all_minilm_l6_v2"
      
      all_minilm_l6_v2:
        id: "sentence-transformers/all-MiniLM-L6-v2"
        dimensions: 384
        max_seq_length: 256
        cache_dir: "./models/text/all-minilm-l6-v2"
      
      all_mpnet_base_v2:
        id: "sentence-transformers/all-mpnet-base-v2"
        dimensions: 768
        max_seq_length: 384
        cache_dir: "./models/text/all-mpnet-base-v2"
    
    code:
      default: "codebert_base"
      
      codebert_base:
        id: "microsoft/codebert-base"
        dimensions: 768
        max_seq_length: 512
        languages: ["python", "java", "javascript", "go"]
        cache_dir: "./models/code/codebert-base"
      
      graphcodebert_base:
        id: "microsoft/graphcodebert-base"
        dimensions: 768
        max_seq_length: 512
        languages: ["python", "java", "javascript", "go"]
        cache_dir: "./models/code/graphcodebert-base"
    
    multilingual:
      default: "e5_large"
      
      e5_large:
        id: "intfloat/multilingual-e5-large"
        dimensions: 1024
        max_seq_length: 512
        cache_dir: "./models/multilingual/e5-large"

  # Modelos generativos
  generation:
    text:
      default: "mistral_7b"
      
      mistral_7b:
        id: "mistralai/Mistral-7B-Instruct-v0.1"
        context_length: 8192
        cache_dir: "./models/generation/mistral-7b"
        hardware:
          min_vram_gb: 16
          recommended_vram_gb: 24
      
      llama2_7b_chat:
        id: "meta-llama/Llama-2-7b-chat-hf"
        context_length: 4096
        cache_dir: "./models/generation/llama2-7b"
        hardware:
          min_vram_gb: 14
          recommended_vram_gb: 16
    
    code:
      default: "codellama_7b"
      
      codellama_7b:
        id: "codellama/CodeLlama-7b-Instruct-hf"
        context_length: 4096
        cache_dir: "./models/generation/codellama-7b"
        hardware:
          min_vram_gb: 16
          recommended_vram_gb: 20

  # Configuración de hardware
  hardware:
    gpu:
      enabled: "${MODELS_GPU_ENABLED:-true}"
      devices: "${CUDA_VISIBLE_DEVICES:-[0]}"
      memory_fraction: 0.8
      precision: "float16"
    
    cpu:
      threads: "${MODELS_CPU_THREADS:-${CPU_COUNT:-8}}"
      memory_limit_gb: "${MODELS_CPU_MEMORY_GB:-16}"
    
    mixed_precision:
      enabled: true
      dtype: "${MODELS_PRECISION:-float16}"
    
    quantization:
      enabled: "${MODELS_QUANTIZATION:-true}"
      backend: "bitsandbytes"
      precision: "int8"

  # Caché y optimización
  cache:
    root_dir: "./models"
    max_size_gb: 50
    eviction_policy: "LRU"
    
    preload:
      enabled: true
      models:
        - text.all_minilm_l6_v2
        - code.codebert_base