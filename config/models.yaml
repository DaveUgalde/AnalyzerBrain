# ============================================
# CONFIGURACIÓN DE MODELOS ML - PROJECT BRAIN
# ============================================

models:

  # ============================================
  # EMBEDDINGS
  # ============================================

  embeddings:

    text:
      all_minilm_l6_v2:
        id: "sentence-transformers/all-MiniLM-L6-v2"
        type: "embedding"
        dimensions: 384
        max_seq_length: 512
        language: "multilingual"
        license: "apache-2.0"
        cache_dir: "./models/all-minilm-l6-v2"

      all_mpnet_base_v2:
        id: "sentence-transformers/all-mpnet-base-v2"
        type: "embedding"
        dimensions: 768
        max_seq_length: 512
        language: "english"
        license: "apache-2.0"
        cache_dir: "./models/all-mpnet-base-v2"

      multi_qa_mpnet_dot_v1:
        id: "sentence-transformers/multi-qa-mpnet-base-dot-v1"
        type: "embedding"
        dimensions: 768
        max_seq_length: 512
        language: "english"
        license: "apache-2.0"
        cache_dir: "./models/multi-qa-mpnet-dot-v1"

      paraphrase_multilingual_mpnet_v2:
        id: "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
        type: "embedding"
        dimensions: 768
        max_seq_length: 512
        language: "multilingual"
        license: "apache-2.0"
        cache_dir: "./models/paraphrase-multilingual-mpnet-v2"

    code:
      codebert_base:
        id: "microsoft/codebert-base"
        type: "embedding"
        dimensions: 768
        max_seq_length: 512
        languages: ["python", "java", "javascript", "go", "ruby", "php"]
        license: "mit"
        cache_dir: "./models/codebert-base"

      graphcodebert_base:
        id: "microsoft/graphcodebert-base"
        type: "embedding"
        dimensions: 768
        max_seq_length: 512
        languages: ["python", "java", "javascript", "go", "ruby", "php"]
        license: "mit"
        cache_dir: "./models/graphcodebert-base"

    multilingual:
      e5_large:
        id: "intfloat/multilingual-e5-large"
        type: "embedding"
        dimensions: 1024
        max_seq_length: 512
        license: "mit"
        cache_dir: "./models/e5-large"

  # ============================================
  # CLASIFICACIÓN
  # ============================================

  classification:

    intent:
      id: "bert-base-multilingual-cased"
      engine: "transformers"
      num_labels: 12
      labels:
        - code_analysis
        - question_answer
        - architecture_review
        - security_audit
        - performance_analysis
        - dependency_check
        - pattern_detection
        - code_generation
        - refactoring_suggestion
        - documentation_request
        - test_generation
        - other
      cache_dir: "./models/intent_classifier"

    language:
      engine: "fasttext"
      model_path: "./models/fasttext/language.bin"
      supported_languages:
        - python
        - javascript
        - typescript
        - java
        - cpp
        - go
        - rust
        - csharp
        - php
        - ruby

  # ============================================
  # GENERACIÓN
  # ============================================

  generation:

    code:
      codellama_7b:
        id: "codellama/CodeLlama-7b-Instruct-hf"
        type: "generative"
        context_length: 4096
        license: "llama2"
        cache_dir: "./models/codellama-7b"
        hardware_required:
          min_vram_gb: 16

      wizardcoder_34b:
        id: "WizardLM/WizardCoder-Python-34B-V1.0"
        type: "generative"
        context_length: 2048
        license: "llama2"
        cache_dir: "./models/wizardcoder-34b"
        hardware_required:
          min_vram_gb: 48
          load_if_available: false

    text:
      mistral_7b:
        id: "mistralai/Mistral-7B-Instruct-v0.1"
        type: "generative"
        context_length: 8192
        license: "apache-2.0"
        cache_dir: "./models/mistral-7b"

      llama2_7b_chat:
        id: "meta-llama/Llama-2-7b-chat-hf"
        type: "generative"
        context_length: 4096
        license: "llama2"
        cache_dir: "./models/llama2-7b"

  # ============================================
  # ANÁLISIS DE CÓDIGO
  # ============================================

  code_analysis:

    smells:
      model: "codebert_base"
      types:
        - long_method
        - large_class
        - duplicate_code
        - complex_method
        - many_parameters
        - feature_envy
        - data_class
        - god_class

    security:
      model: "graphcodebert_base"
      vulnerabilities:
        - sql_injection
        - xss
        - command_injection
        - path_traversal
        - insecure_deserialization
        - weak_cryptography

    complexity:
      engine: "classical_ml"
      model: "random_forest"
      features:
        - line_count
        - parameter_count
        - nesting_depth
        - control_flow_statements

  # ============================================
  # OPTIMIZACIÓN
  # ============================================

  optimization:

    quantization:
      enabled: true
      backend: "bitsandbytes"
      precision: "int8"

    pruning:
      enabled: false

    distillation:
      enabled: false

  # ============================================
  # CACHE
  # ============================================

  cache:
    root_dir: "./models"
    max_size_gb: 50
    eviction_policy: "lru"
    preload:
      enabled: true
      models:
        - all_minilm_l6_v2
        - codebert_base

  # ============================================
  # HARDWARE
  # ============================================

  hardware:
    gpu:
      enabled: true
      devices: [0]
      memory_fraction: 0.8

    cpu:
      threads: 8

    mixed_precision:
      enabled: true
      dtype: "float16"
