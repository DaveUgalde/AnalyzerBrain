Documento de Arquitectura Unificado: Project Brain (AnalyzerBrain)

1. VISI√ìN Y ALCANCE SIST√âMICO

1.1 Prop√≥sito del Sistema

Project Brain es un sistema de inteligencia artificial especializado en la comprensi√≥n, an√°lisis y gesti√≥n evolutiva de proyectos de software. Act√∫a como un cerebro colectivo persistente que supera las limitaciones de contexto de los LLMs tradicionales, proporcionando memoria infinita, aprendizaje incremental y comprensi√≥n estructural profunda del c√≥digo. El sistema es capaz de analizar proyectos en m√∫ltiples lenguajes, mantener un historial completo de cambios, proporcionar recomendaciones t√©cnicas espec√≠ficas y evolucionar con el proyecto.

1.2 Objetivos Principales

Objetivo	Descripci√≥n T√©cnica	M√©trica de √âxito
Memoria Infinita	Almacenamiento persistente de an√°lisis hist√≥ricos sin p√©rdida por l√≠mites de contexto	Retenci√≥n del 100% del conocimiento adquirido
Comprensi√≥n Profunda	An√°lisis a nivel de archivos, funciones, clases, dependencias y patrones	Precisi√≥n > 90% en identificaci√≥n de entidades
Aprendizaje Continuo	Mejora de comprensi√≥n con cada interacci√≥n sin sobrescribir conocimiento	Incremento del 5% mensual en precisi√≥n
Razonamiento Estructural	Entendimiento de arquitecturas, dependencias y patrones de dise√±o	Detecci√≥n del 95% de dependencias cr√≠ticas
Adaptaci√≥n Din√°mica	Actualizaci√≥n autom√°tica ante cambios en el proyecto	Tiempo de actualizaci√≥n < 30 segundos por cambio
Interfaz Multimodal	CLI, API REST, WebSocket, gRPC e interfaz web	Disponibilidad > 99.5% en todas las interfaces
An√°lisis Predictivo	Predicci√≥n de problemas y sugerencias proactivas	Reducci√≥n del 30% en bugs introducidos
1.3 Problemas que Resuelve

L√≠mite de contexto en LLMs: Persistencia de memoria entre sesiones mediante bases vectoriales y de grafos
P√©rdida de an√°lisis: Retenci√≥n de conocimientos previos con versionado y historial completo
Falta de estructura: Comprensi√≥n organizada del proyecto mediante grafos de conocimiento
An√°lisis fragmentado: Visi√≥n hol√≠stica del sistema mediante agentes especializados colaborativos
Actualizaci√≥n manual: Detecci√≥n autom√°tica de cambios con re-an√°lisis incremental
Desconocimiento del estado: Monitoreo continuo de m√©tricas de calidad y deuda t√©cnica
1.4 An√°lisis de Potencial y Efectividad Esperada

Potencial del Sistema: Project Brain representa la evoluci√≥n de las herramientas de an√°lisis de c√≥digo hacia sistemas cognitivos completos. Al combinar an√°lisis est√°tico multi-lenguaje, representaciones vectoriales sem√°nticas, grafos de conocimiento y agentes especializados, el sistema puede:

Comprender proyectos complejos (1M+ LOC) en m√∫ltiples lenguajes simult√°neamente
Proporcionar respuestas contextuales precisas basadas en el estado actual e hist√≥rico del proyecto
Predecir problemas antes de que ocurran mediante an√°lisis de patrones hist√≥ricos
Recomendar mejoras espec√≠ficas a nivel de c√≥digo, dise√±o y arquitectura
Adaptarse al estilo del equipo mediante aprendizaje de interacciones previas
Efectividad Esperada:

Reducci√≥n del 50% en tiempo de onboarding de nuevos desarrolladores
Disminuci√≥n del 40% en bugs causados por mal entendimiento del c√≥digo
Aumento del 60% en reutilizaci√≥n de c√≥digo existente
Mejora del 70% en documentaci√≥n autom√°tica y actualizada
ROI positivo en 6 meses para equipos de >10 desarrolladores
2. ARQUITECTURA GENERAL

2.1 Patr√≥n Arquitect√≥nico

Arquitectura H√≠brida: Microkernel + Sistema de Agentes + Base de Conocimiento Centralizada

text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                             CAPA DE PRESENTACI√ìN                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ    CLI      ‚îÇ   API REST   ‚îÇ  WebSocket   ‚îÇ    gRPC      ‚îÇ   Web UI        ‚îÇ
‚îÇ  (click)    ‚îÇ  (FastAPI)   ‚îÇ (real-time)  ‚îÇ (high-perf)  ‚îÇ  (Streamlit)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          CAPA DE ORQUESTACI√ìN                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇWorkflow Mng ‚îÇ Task Scheduler‚îÇPipeline Orch.‚îÇ         Event Bus               ‚îÇ
‚îÇ(Prefect)    ‚îÇ (Celery)     ‚îÇ (Kedro)      ‚îÇ (Redis Pub/Sub)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      SISTEMA DE AGENTES ESPECIALIZADOS                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Arquitecto  ‚îÇ  Detective   ‚îÇ  Analista    ‚îÇ   Curador    ‚îÇ     Q&A         ‚îÇ
‚îÇ (patrones)  ‚îÇ (problemas)  ‚îÇ (m√©tricas)   ‚îÇ(conocimiento)‚îÇ (respuestas)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          N√öCLEO DE INTELIGENCIA                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Red Neuronal‚îÇ  Memoria     ‚îÇ  An√°lisis    ‚îÇ         Aprendizaje             ‚îÇ
‚îÇ (GNN)       ‚îÇ(vector+grafo)‚îÇ (profundo)   ‚îÇ     (incremental+RL)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          PIPELINE DE DATOS                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Ingesti√≥n  ‚îÇProcesamiento ‚îÇAlmacenamiento‚îÇ            Cache                ‚îÇ
‚îÇ (scanner)   ‚îÇ(parsing+emb) ‚îÇ  (multi-DB)  ‚îÇ        (Redis+Memcached)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          SISTEMA DE CONSULTAS                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ     NLP     ‚îÇ Recuperaci√≥n ‚îÇ Razonamiento ‚îÇ           Respuesta             ‚îÇ
‚îÇ(intent+NER) ‚îÇ(vector+grafo)‚îÇ(chain+agents)‚îÇ      (synthesis+formatting)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
2.2 Patrones de Dise√±o Aplicados

Patr√≥n	M√≥dulo	Implementaci√≥n	Beneficio
Microkernel	Core	BrainOrchestrator + plugins	Extensibilidad sin modificar core
Strategy	Analysis Engine	Diferentes parsers por lenguaje	Intercambiabilidad de algoritmos
Observer	Event System	EventBus con suscripciones	Comunicaci√≥n desacoplada
Repository	Memory System	VectorStore + GraphStore abstracciones	Independencia de almacenamiento
Factory Method	Agent System	AgentFactory con registro din√°mico	Creaci√≥n flexible de agentes
Chain of Responsibility	Query Pipeline	InferenceChain con handlers	Procesamiento modular de preguntas
CQRS	Memory System	Separaci√≥n lecturas/escrituras	Optimizaci√≥n por caso de uso
SAGA	Orchestration	Transacciones distribuidas	Consistencia en operaciones complejas
Circuit Breaker	External APIs	APIClient con fallback	Resiliencia a fallos externos
2.3 Principios de Dise√±o

Inmutabilidad del Conocimiento: El conocimiento nunca se elimina, solo se refina y versiona
Separaci√≥n de Responsabilidades: Cada m√≥dulo tiene una √∫nica responsabilidad clara y bien definida
Desacoplamiento por Eventos: Comunicaci√≥n as√≠ncrona entre componentes mediante bus de eventos
Extensibilidad por Dise√±o: Capacidad de a√±adir nuevos agentes, parsers y almacenamientos sin modificar core
Observabilidad Total: M√©tricas, logs y traces en todos los niveles del sistema
Fall Gracefully: El sistema debe degradar funcionalidades sin colapsar completamente
Security by Design: Autenticaci√≥n, autorizaci√≥n y sanitizaci√≥n en cada capa
3. ESTRUCTURA DE PROYECTO COMPLETA

3.1 √Årbol de Directorios Ra√≠z

text
project_brain/
‚îú‚îÄ‚îÄ üìÑ README.md                          # Documentaci√≥n principal
‚îú‚îÄ‚îÄ üìÑ LICENSE                            # Licencia MIT
‚îú‚îÄ‚îÄ üìÑ pyproject.toml                     # Configuraci√≥n moderna de paquete
‚îú‚îÄ‚îÄ üìÑ requirements/                       # Dependencias categorizadas
‚îÇ   ‚îú‚îÄ‚îÄ base.txt                         # Dependencias core
‚îÇ   ‚îú‚îÄ‚îÄ ml.txt                           # ML y embeddings
‚îÇ   ‚îú‚îÄ‚îÄ databases.txt                    # Bases de datos
‚îÇ   ‚îú‚îÄ‚îÄ agents.txt                       # Agentes y LLMs
‚îÇ   ‚îú‚îÄ‚îÄ api.txt                          # APIs y web
‚îÇ   ‚îú‚îÄ‚îÄ dev.txt                          # Desarrollo y testing
‚îÇ   ‚îî‚îÄ‚îÄ prod.txt                         # Producci√≥n espec√≠fica
‚îú‚îÄ‚îÄ ‚öôÔ∏è .env.example                       # Variables de entorno ejemplo
‚îú‚îÄ‚îÄ ‚öôÔ∏è config/                            # Configuraci√≥n centralizada
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ system.yaml                      # Configuraci√≥n general del sistema
‚îÇ   ‚îú‚îÄ‚îÄ models.yaml                      # Configuraci√≥n de modelos ML
‚îÇ   ‚îú‚îÄ‚îÄ agents.yaml                      # Configuraci√≥n de agentes
‚îÇ   ‚îú‚îÄ‚îÄ databases.yaml                   # Configuraci√≥n de bases de datos
‚îÇ   ‚îî‚îÄ‚îÄ api.yaml                         # Configuraci√≥n de APIs
‚îú‚îÄ‚îÄ üìÅ src/                               # C√≥digo fuente principal
‚îÇ   ‚îú‚îÄ‚îÄ üì¶ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ üöÄ main.py                       # Punto de entrada principal
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ core/                          # N√∫cleo del sistema (15%)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ indexer/                       # Indexaci√≥n y parsing (25%)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ embeddings/                    # Representaci√≥n vectorial (15%)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ graph/                         # Grafo de conocimiento (10%)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ memory/                        # Sistema de memoria (10%)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ agents/                        # Agentes IA (15%)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ api/                           # Interfaces (5%)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ learning/                      # Aprendizaje (3%)
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ utils/                         # Utilidades (2%)
‚îú‚îÄ‚îÄ üìÅ data/                             # Datos generados (no en git)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ projects/                      # Proyectos analizados
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ embeddings/                    # Base vectorial ChromaDB
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ graph_exports/                 # Exportaciones Neo4j
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ cache/                         # Cach√© distribuida
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ state/                         # Estado del sistema
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ backups/                       # Backups autom√°ticos
‚îú‚îÄ‚îÄ üìÅ tests/                            # Pruebas (30% cobertura inicial)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ unit/                          # Pruebas unitarias
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ integration/                   # Pruebas de integraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ e2e/                           # Pruebas end-to-end
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ performance/                   # Pruebas de performance
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ fixtures/                      # Datos de prueba
‚îÇ   ‚îî‚îÄ‚îÄ conftest.py                      # Configuraci√≥n pytest
‚îú‚îÄ‚îÄ üìÅ scripts/                          # Scripts de utilidad
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö° init_project.py               # Inicializaci√≥n del sistema
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö° analyze_project.py            # An√°lisis de proyecto
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö° query_project.py              # Consulta desde CLI
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö° export_knowledge.py           # Exportaci√≥n de conocimiento
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö° monitor_system.py             # Monitoreo del sistema
‚îÇ   ‚îî‚îÄ‚îÄ ‚ö° backup_restore.py             # Backup y restauraci√≥n
‚îú‚îÄ‚îÄ üìÅ docs/                             # Documentaci√≥n completa
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ architecture/                 # Documentaci√≥n de arquitectura
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ api/                          # Documentaci√≥n de API
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ user_guide/                   # Gu√≠a de usuario
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ developer/                    # Gu√≠a de desarrollador
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ deployment/                   # Gu√≠a de despliegue
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ examples/                     # Ejemplos de uso
‚îú‚îÄ‚îÄ üìÅ deployments/                      # Configuraciones de despliegue
‚îÇ   ‚îú‚îÄ‚îÄ docker/                          # Configuraci√≥n Docker
‚îÇ   ‚îú‚îÄ‚îÄ kubernetes/                      # Configuraci√≥n Kubernetes
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml               # Desarrollo local
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.prod.yml          # Producci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ helm/                           # Charts Helm
‚îú‚îÄ‚îÄ üìÅ monitoring/                       # Configuraci√≥n de monitoreo
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/                      # Configuraci√≥n Prometheus
‚îÇ   ‚îú‚îÄ‚îÄ grafana/                         # Dashboards Grafana
‚îÇ   ‚îú‚îÄ‚îÄ alerts/                          # Reglas de alertas
‚îÇ   ‚îî‚îÄ‚îÄ loki/                           # Configuraci√≥n logging
‚îî‚îÄ‚îÄ üìÅ .github/                          # GitHub workflows
    ‚îú‚îÄ‚îÄ workflows/
    ‚îÇ   ‚îú‚îÄ‚îÄ ci.yml                       # CI continuo
    ‚îÇ   ‚îú‚îÄ‚îÄ cd.yml                       # CD continuo
    ‚îÇ   ‚îú‚îÄ‚îÄ tests.yml                    # Ejecuci√≥n de tests
    ‚îÇ   ‚îî‚îÄ‚îÄ security.yml                 # Escaneo de seguridad
    ‚îî‚îÄ‚îÄ dependabot.yml                   # Actualizaciones de dependencias
3.2 Especificaci√≥n de M√≥dulos con Distribuci√≥n de Responsabilidad

3.2.1 M√≥dulo: core/ (15% del sistema)

Responsabilidad: Orquestaci√≥n principal, gesti√≥n de estado, configuraci√≥n y utilidades base.

text
core/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ orchestrator.py                    # BrainOrchestrator - Coordinador principal
‚îú‚îÄ‚îÄ system_state.py                    # SystemStateManager - Gesti√≥n de estado
‚îú‚îÄ‚îÄ workflow_manager.py                # WorkflowOrchestrator - Orquestaci√≥n de flujos
‚îú‚îÄ‚îÄ event_bus.py                       # EventBus - Sistema de eventos
‚îú‚îÄ‚îÄ config_manager.py                  # ConfigManager - Gesti√≥n de configuraci√≥n
‚îú‚îÄ‚îÄ dependency_injector.py             # DependencyInjector - Inyecci√≥n de dependencias
‚îú‚îÄ‚îÄ plugin_manager.py                  # PluginManager - Gesti√≥n de plugins
‚îú‚îÄ‚îÄ health_check.py                    # HealthCheck - Verificaci√≥n de salud
‚îî‚îÄ‚îÄ exceptions.py                      # Excepciones personalizadas del sistema
3.2.2 M√≥dulo: indexer/ (25% del sistema)

Responsabilidad: An√°lisis inicial, parsing de c√≥digo, extracci√≥n de estructura y detecci√≥n de cambios.

text
indexer/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ project_scanner.py                 # ProjectScanner - Escaneo de proyectos
‚îú‚îÄ‚îÄ file_processor.py                  # FileProcessor - Procesamiento de archivos
‚îú‚îÄ‚îÄ multi_language_parser.py           # MultiLanguageParser - Parsers multi-lenguaje
‚îú‚îÄ‚îÄ entity_extractor.py                # EntityExtractor - Extracci√≥n de entidades
‚îú‚îÄ‚îÄ dependency_mapper.py               # DependencyMapper - Mapeo de dependencias
‚îú‚îÄ‚îÄ change_detector.py                 # ChangeDetector - Detecci√≥n de cambios
‚îú‚îÄ‚îÄ version_tracker.py                 # VersionTracker - Seguimiento de versiones
‚îú‚îÄ‚îÄ quality_analyzer.py                # QualityAnalyzer - An√°lisis de calidad
‚îî‚îÄ‚îÄ pattern_detector.py                # PatternDetector - Detecci√≥n de patrones
3.2.3 M√≥dulo: embeddings/ (15% del sistema)

Responsabilidad: Generaci√≥n, almacenamiento y b√∫squeda de embeddings vectoriales.

text
embeddings/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ embedding_models.py                # EmbeddingModels - Modelos de embeddings
‚îú‚îÄ‚îÄ embedding_generator.py             # EmbeddingGenerator - Generaci√≥n de embeddings
‚îú‚îÄ‚îÄ vector_store.py                    # VectorStore - Almacenamiento vectorial
‚îú‚îÄ‚îÄ semantic_search.py                 # SemanticSearch - B√∫squeda sem√°ntica
‚îú‚îÄ‚îÄ embedding_cache.py                 # EmbeddingCache - Cach√© de embeddings
‚îú‚îÄ‚îÄ similarity_calculator.py           # SimilarityCalculator - C√°lculo de similitudes
‚îî‚îÄ‚îÄ dimensionality_reducer.py          # DimensionalityReducer - Reducci√≥n dimensional
3.2.4 M√≥dulo: graph/ (10% del sistema)

Responsabilidad: Construcci√≥n, gesti√≥n y consulta del grafo de conocimiento.

text
graph/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ knowledge_graph.py                 # KnowledgeGraph - Grafo de conocimiento principal
‚îú‚îÄ‚îÄ graph_builder.py                   # GraphBuilder - Constructor de grafos
‚îú‚îÄ‚îÄ graph_query_engine.py              # GraphQueryEngine - Motor de consultas
‚îú‚îÄ‚îÄ graph_traverser.py                 # GraphTraverser - Navegaci√≥n de grafos
‚îú‚îÄ‚îÄ graph_analytics.py                 # GraphAnalytics - An√°lisis de grafos
‚îú‚îÄ‚îÄ graph_exporter.py                  # GraphExporter - Exportaci√≥n de grafos
‚îú‚îÄ‚îÄ schema_manager.py                  # SchemaManager - Gesti√≥n de esquemas
‚îî‚îÄ‚îÄ consistency_checker.py             # ConsistencyChecker - Verificaci√≥n de consistencia
3.2.5 M√≥dulo: memory/ (10% del sistema)

Responsabilidad: Sistemas de memoria persistente y cach√©.

text
memory/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ memory_hierarchy.py                # MemoryHierarchy - Jerarqu√≠a de memoria
‚îú‚îÄ‚îÄ episodic_memory.py                 # EpisodicMemory - Memoria epis√≥dica
‚îú‚îÄ‚îÄ semantic_memory.py                 # SemanticMemory - Memoria sem√°ntica
‚îú‚îÄ‚îÄ working_memory.py                  # WorkingMemory - Memoria de trabajo
‚îú‚îÄ‚îÄ memory_consolidator.py             # MemoryConsolidator - Consolidaci√≥n de memoria
‚îú‚îÄ‚îÄ cache_manager.py                   # CacheManager - Gesti√≥n de cach√©
‚îú‚îÄ‚îÄ memory_retriever.py                # MemoryRetriever - Recuperaci√≥n de memoria
‚îî‚îÄ‚îÄ memory_cleaner.py                  # MemoryCleaner - Limpieza de memoria
3.2.6 M√≥dulo: agents/ (15% del sistema)

Responsabilidad: Agentes de IA especializados y su coordinaci√≥n.

text
agents/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ base_agent.py                      # BaseAgent - Clase base para todos los agentes
‚îú‚îÄ‚îÄ agent_factory.py                   # AgentFactory - F√°brica de agentes
‚îú‚îÄ‚îÄ agent_orchestrator.py              # AgentOrchestrator - Coordinaci√≥n de agentes
‚îú‚îÄ‚îÄ code_analyzer_agent.py             # CodeAnalyzerAgent - An√°lisis de c√≥digo
‚îú‚îÄ‚îÄ architect_agent.py                 # ArchitectAgent - An√°lisis arquitect√≥nico
‚îú‚îÄ‚îÄ detective_agent.py                 # DetectiveAgent - Investigaci√≥n de problemas
‚îú‚îÄ‚îÄ qa_agent.py                        # QuestionAnsweringAgent - Preguntas y respuestas
‚îú‚îÄ‚îÄ curator_agent.py                   # CuratorAgent - Curaci√≥n de conocimiento
‚îú‚îÄ‚îÄ analyst_agent.py                   # AnalystAgent - An√°lisis de m√©tricas
‚îú‚îÄ‚îÄ security_agent.py                  # SecurityAgent - An√°lisis de seguridad
‚îú‚îÄ‚îÄ learning_agent.py                  # LearningAgent - Agente de aprendizaje
‚îî‚îÄ‚îÄ collaboration_protocol.py          # CollaborationProtocol - Protocolo de colaboraci√≥n
3.2.7 M√≥dulo: api/ (5% del sistema)

Responsabilidad: Interfaces de comunicaci√≥n externa.

text
api/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ server.py                          # APIServer - Servidor principal
‚îú‚îÄ‚îÄ rest_api.py                        # REST API - Endpoints REST
‚îú‚îÄ‚îÄ websocket_api.py                   # WebSocket API - Comunicaci√≥n en tiempo real
‚îú‚îÄ‚îÄ grpc_api.py                        # gRPC API - Alta performance
‚îú‚îÄ‚îÄ cli_interface.py                   # CLI - Interfaz de l√≠nea de comandos
‚îú‚îÄ‚îÄ web_ui.py                          # Web UI - Interfaz web
‚îú‚îÄ‚îÄ authentication.py                  # Authentication - Autenticaci√≥n
‚îú‚îÄ‚îÄ rate_limiter.py                    # RateLimiter - Limitaci√≥n de tasa
‚îî‚îÄ‚îÄ request_validator.py               # RequestValidator - Validaci√≥n de peticiones
3.2.8 M√≥dulo: learning/ (3% del sistema)

Responsabilidad: Sistemas de aprendizaje incremental y mejora continua.

text
learning/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ feedback_loop.py                   # FeedbackLoop - Bucle de retroalimentaci√≥n
‚îú‚îÄ‚îÄ incremental_learner.py             # IncrementalLearner - Aprendizaje incremental
‚îú‚îÄ‚îÄ reinforcement_learner.py           # ReinforcementLearner - Aprendizaje por refuerzo
‚îú‚îÄ‚îÄ knowledge_refiner.py               # KnowledgeRefiner - Refinamiento de conocimiento
‚îú‚îÄ‚îÄ adaptation_engine.py               # AdaptationEngine - Motor de adaptaci√≥n
‚îú‚îÄ‚îÄ forgetting_mechanism.py            # ForgettingMechanism - Mecanismo de olvido
‚îî‚îÄ‚îÄ learning_evaluator.py              # LearningEvaluator - Evaluaci√≥n de aprendizaje
3.2.9 M√≥dulo: utils/ (2% del sistema)

Responsabilidad: Utilidades compartidas y helpers.

text
utils/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ file_utils.py                      # FileUtils - Utilidades de archivos
‚îú‚îÄ‚îÄ text_processing.py                 # TextProcessing - Procesamiento de texto
‚îú‚îÄ‚îÄ parallel_processing.py             # ParallelProcessing - Procesamiento paralelo
‚îú‚îÄ‚îÄ security_utils.py                  # SecurityUtils - Utilidades de seguridad
‚îú‚îÄ‚îÄ logging_config.py                  # LoggingConfig - Configuraci√≥n de logging
‚îú‚îÄ‚îÄ metrics_collector.py               # MetricsCollector - Colecci√≥n de m√©tricas
‚îú‚îÄ‚îÄ serialization.py                   # Serialization - Serializaci√≥n de datos
‚îî‚îÄ‚îÄ validation.py                      # Validation - Validaci√≥n de datos
4. ESPECIFICACIONES DETALLADAS POR ARCHIVO Y FUNCI√ìN

4.1 Archivo: src/core/orchestrator.py

python
"""
BrainOrchestrator - Coordinador principal del sistema.
Responsable de orquestar todos los componentes, gestionar flujos de trabajo
y mantener el estado global del sistema.
"""

from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import uuid
from datetime import datetime
from pydantic import BaseModel, Field, validator
from .system_state import SystemState
from .event_bus import EventBus, EventType
from .exceptions import BrainException, ValidationError, TimeoutError

class SystemMode(Enum):
    """Modos de operaci√≥n del sistema."""
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION = "production"
    MAINTENANCE = "maintenance"

class OperationPriority(Enum):
    """Prioridades de operaci√≥n."""
    CRITICAL = 0    # An√°lisis en tiempo real, respuestas a usuarios
    HIGH = 1        # Procesamiento de cambios, aprendizaje
    MEDIUM = 2      # An√°lisis programado, mantenimiento
    LOW = 3         # Tareas en background, limpieza

@dataclass
class OrchestratorConfig:
    """Configuraci√≥n del orquestador."""
    system_mode: SystemMode = SystemMode.DEVELOPMENT
    max_concurrent_operations: int = 10
    operation_timeout_seconds: Dict[OperationPriority, int] = field(
        default_factory=lambda: {
            OperationPriority.CRITICAL: 30,
            OperationPriority.HIGH: 300,
            OperationPriority.MEDIUM: 1800,
            OperationPriority.LOW: 3600
        }
    )
    enable_learning: bool = True
    enable_monitoring: bool = True
    enable_backup: bool = True
    plugins_directory: str = "./plugins"

class ProjectContext(BaseModel):
    """Contexto de un proyecto para operaciones."""
    project_id: str = Field(..., description="ID √∫nico del proyecto")
    project_path: str = Field(..., description="Ruta al proyecto")
    language: Optional[str] = Field(None, description="Lenguaje principal")
    analysis_depth: str = Field("comprehensive", description="Profundidad de an√°lisis")
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    
    class Config:
        arbitrary_types_allowed = True

class OperationRequest(BaseModel):
    """Solicitud de operaci√≥n al orquestador."""
    operation_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    operation_type: str = Field(..., description="Tipo de operaci√≥n")
    priority: OperationPriority = Field(OperationPriority.MEDIUM)
    context: Dict[str, Any] = Field(default_factory=dict)
    timeout_seconds: Optional[int] = Field(None)
    callback_url: Optional[str] = Field(None)
    
    @validator('operation_type')
    def validate_operation_type(cls, v):
        allowed_types = {'analyze_project', 'process_question', 'detect_changes', 
                        'learn_from_feedback', 'export_knowledge', 'system_status'}
        if v not in allowed_types:
            raise ValueError(f"Operation type must be one of {allowed_types}")
        return v

class OperationResult(BaseModel):
    """Resultado de una operaci√≥n."""
    operation_id: str
    success: bool
    data: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    warnings: List[str] = Field(default_factory=list)
    metrics: Dict[str, Any] = Field(default_factory=dict)
    processing_time_ms: float = 0.0
    timestamp: datetime = Field(default_factory=datetime.now)

class BrainOrchestrator:
    """
    Clase principal de orquestaci√≥n del sistema.
    
    Responsabilidades:
    1. Coordinar todos los componentes del sistema
    2. Gestionar flujos de trabajo complejos
    3. Manejar concurrencia y prioridades
    4. Proporcionar interfaz unificada para operaciones
    5. Gestionar estado y recuperaci√≥n de errores
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """
        Inicializa el orquestador del sistema.
        
        Args:
            config_path: Ruta al archivo de configuraci√≥n (opcional)
            
        Raises:
            ValidationError: Si la configuraci√≥n es inv√°lida
            BrainException: Si hay errores de inicializaci√≥n
        """
        self._config = self._load_config(config_path)
        self._state = SystemState()
        self._event_bus = EventBus()
        self._operations_queue = asyncio.PriorityQueue()
        self._active_operations: Dict[str, asyncio.Task] = {}
        self._components: Dict[str, Any] = {}
        self._plugins: Dict[str, Any] = {}
        self._is_running = False
        self._start_time = datetime.now()
        
        # M√©tricas
        self._metrics = {
            "operations_completed": 0,
            "operations_failed": 0,
            "total_processing_time_ms": 0.0,
            "avg_response_time_ms": 0.0,
            "concurrent_operations": 0
        }
        
    async def initialize(self) -> bool:
        """
        Inicializa todos los componentes del sistema.
        
        Returns:
            bool: True si la inicializaci√≥n fue exitosa
            
        Raises:
            BrainException: Si alg√∫n componente falla al inicializar
        """
        try:
            # 1. Inicializar sistema de eventos
            await self._event_bus.initialize()
            
            # 2. Inicializar componentes core
            components_order = [
                "config_manager",
                "dependency_injector", 
                "plugin_manager",
                "health_check",
                "workflow_manager"
            ]
            
            for component_name in components_order:
                component = await self._initialize_component(component_name)
                self._components[component_name] = component
            
            # 3. Cargar plugins
            await self._load_plugins()
            
            # 4. Iniciar workers de procesamiento
            await self._start_workers()
            
            # 5. Iniciar monitoreo
            if self._config.enable_monitoring:
                await self._start_monitoring()
            
            self._is_running = True
            self._state.set_ready()
            
            await self._event_bus.publish(
                EventType.SYSTEM_STARTED,
                {"timestamp": datetime.now(), "version": "1.0.0"}
            )
            
            return True
            
        except Exception as e:
            await self._event_bus.publish(
                EventType.SYSTEM_ERROR,
                {"error": str(e), "component": "orchestrator"}
            )
            raise BrainException(f"Failed to initialize orchestrator: {e}")
    
    async def process_operation(self, request: OperationRequest) -> OperationResult:
        """
        Procesa una operaci√≥n en el sistema.
        
        Args:
            request: Solicitud de operaci√≥n
            
        Returns:
            OperationResult: Resultado de la operaci√≥n
            
        Raises:
            ValidationError: Si la solicitud es inv√°lida
            TimeoutError: Si la operaci√≥n excede el timeout
        """
        start_time = datetime.now()
        operation_id = request.operation_id
        
        try:
            # Validar solicitud
            self._validate_operation_request(request)
            
            # Publicar evento de inicio
            await self._event_bus.publish(
                EventType.OPERATION_STARTED,
                {"operation_id": operation_id, "type": request.operation_type}
            )
            
            # Procesar seg√∫n tipo
            if request.operation_type == "analyze_project":
                result = await self._analyze_project(request.context)
            elif request.operation_type == "process_question":
                result = await self._process_question(request.context)
            elif request.operation_type == "detect_changes":
                result = await self._detect_changes(request.context)
            elif request.operation_type == "learn_from_feedback":
                result = await self._learn_from_feedback(request.context)
            elif request.operation_type == "export_knowledge":
                result = await self._export_knowledge(request.context)
            elif request.operation_type == "system_status":
                result = await self._get_system_status()
            else:
                raise ValidationError(f"Unknown operation type: {request.operation_type}")
            
            # Calcular m√©tricas
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            self._update_metrics(success=True, processing_time=processing_time)
            
            # Publicar evento de completado
            await self._event_bus.publish(
                EventType.OPERATION_COMPLETED,
                {
                    "operation_id": operation_id,
                    "success": True,
                    "processing_time_ms": processing_time
                }
            )
            
            return OperationResult(
                operation_id=operation_id,
                success=True,
                data=result,
                processing_time_ms=processing_time
            )
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            self._update_metrics(success=False, processing_time=processing_time)
            
            await self._event_bus.publish(
                EventType.OPERATION_FAILED,
                {
                    "operation_id": operation_id,
                    "error": str(e),
                    "processing_time_ms": processing_time
                }
            )
            
            return OperationResult(
                operation_id=operation_id,
                success=False,
                error=str(e),
                processing_time_ms=processing_time
            )
    
    async def analyze_project(self, project_path: str, 
                            options: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Ejecuta an√°lisis completo de un proyecto.
        
        Args:
            project_path: Ruta al proyecto a analizar
            options: Opciones de an√°lisis (profundidad, lenguajes, etc.)
            
        Returns:
            Dict con resultados del an√°lisis
            
        Example:
            {
                "project_id": "proj_123",
                "status": "completed",
                "files_analyzed": 150,
                "entities_extracted": 1200,
                "analysis_time_seconds": 45.2,
                "findings": [...],
                "recommendations": [...]
            }
        """
        request = OperationRequest(
            operation_type="analyze_project",
            priority=OperationPriority.HIGH,
            context={
                "project_path": project_path,
                "options": options or {},
                "timestamp": datetime.now().isoformat()
            }
        )
        
        result = await self.process_operation(request)
        
        if not result.success:
            raise BrainException(f"Project analysis failed: {result.error}")
        
        return result.data
    
    async def ask_question(self, question: str, 
                          project_id: Optional[str] = None,
                          context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Procesa una pregunta sobre un proyecto.
        
        Args:
            question: Pregunta en lenguaje natural
            project_id: ID del proyecto (opcional si ya hay contexto)
            context: Contexto adicional
            
        Returns:
            Dict con respuesta estructurada
            
        Example:
            {
                "answer": "La funci√≥n process_data se define en...",
                "confidence": 0.92,
                "sources": [...],
                "reasoning_chain": [...],
                "suggested_followups": [...]
            }
        """
        request = OperationRequest(
            operation_type="process_question",
            priority=OperationPriority.CRITICAL,
            context={
                "question": question,
                "project_id": project_id,
                "additional_context": context or {},
                "session_id": str(uuid.uuid4())
            },
            timeout_seconds=30  # M√°ximo 30 segundos para respuestas
        )
        
        result = await self.process_operation(request)
        
        if not result.success:
            # Fallback a respuesta b√°sica si hay error
            return {
                "answer": "Lo siento, hubo un error procesando tu pregunta.",
                "confidence": 0.0,
                "sources": [],
                "error": result.error
            }
        
        return result.data
    
    async def shutdown(self, force: bool = False) -> bool:
        """
        Apaga el sistema de manera controlada.
        
        Args:
            force: Si True, fuerza el apagado sin esperar operaciones
            
        Returns:
            bool: True si el apagado fue exitoso
        """
        try:
            # Publicar evento de apagado
            await self._event_bus.publish(
                EventType.SYSTEM_SHUTDOWN_STARTED,
                {"timestamp": datetime.now(), "force": force}
            )
            
            # Detener aceptaci√≥n de nuevas operaciones
            self._is_running = False
            
            # Esperar operaciones pendientes (si no es forzado)
            if not force and self._active_operations:
                await asyncio.sleep(5)  # Dar tiempo para completar
            
            # Apagar componentes en orden inverso
            shutdown_order = reversed(list(self._components.keys()))
            
            for component_name in shutdown_order:
                component = self._components[component_name]
                if hasattr(component, 'shutdown'):
                    await component.shutdown()
            
            # Apagar event bus
            await self._event_bus.shutdown()
            
            # Guardar estado
            await self._state.save()
            
            # Publicar evento de apagado completado
            await self._event_bus.publish(
                EventType.SYSTEM_SHUTDOWN_COMPLETED,
                {"timestamp": datetime.now()}
            )
            
            return True
            
        except Exception as e:
            # En caso de error, loguear pero continuar con apagado
            await self._event_bus.publish(
                EventType.SYSTEM_ERROR,
                {"error": str(e), "context": "shutdown"}
            )
            
            if force:
                return True
            raise BrainException(f"Error during shutdown: {e}")
    
    # M√©todos privados de implementaci√≥n
    
    async def _analyze_project(self, context: Dict) -> Dict[str, Any]:
        """Implementaci√≥n espec√≠fica de an√°lisis de proyecto."""
        # Este m√©todo ser√≠a implementado con la l√≥gica real
        # Por ahora retornamos estructura esperada
        return {
            "project_id": context.get("project_id", "temp_id"),
            "status": "completed",
            "files_analyzed": 0,
            "entities_extracted": 0,
            "analysis_time_seconds": 0.0,
            "findings": [],
            "recommendations": []
        }
    
    async def _process_question(self, context: Dict) -> Dict[str, Any]:
        """Implementaci√≥n espec√≠fica de procesamiento de preguntas."""
        return {
            "answer": "Respuesta de ejemplo",
            "confidence": 0.9,
            "sources": [],
            "reasoning_chain": [],
            "suggested_followups": []
        }
    
    async def _detect_changes(self, context: Dict) -> Dict[str, Any]:
        """Implementaci√≥n espec√≠fica de detecci√≥n de cambios."""
        return {
            "changes_detected": [],
            "files_modified": 0,
            "impact_analysis": {}
        }
    
    async def _learn_from_feedback(self, context: Dict) -> Dict[str, Any]:
        """Implementaci√≥n espec√≠fica de aprendizaje de feedback."""
        return {
            "learning_applied": True,
            "confidence_updates": 0,
            "new_patterns": []
        }
    
    async def _export_knowledge(self, context: Dict) -> Dict[str, Any]:
        """Implementaci√≥n espec√≠fica de exportaci√≥n de conocimiento."""
        return {
            "export_format": context.get("format", "json"),
            "size_bytes": 0,
            "entities_exported": 0
        }
    
    async def _get_system_status(self) -> Dict[str, Any]:
        """Obtiene estado completo del sistema."""
        return {
            "status": "running" if self._is_running else "stopped",
            "uptime_seconds": (datetime.now() - self._start_time).total_seconds(),
            "components": {name: "healthy" for name in self._components},
            "metrics": self._metrics,
            "active_operations": len(self._active_operations),
            "system_mode": self._config.system_mode.value
        }
    
    def _validate_operation_request(self, request: OperationRequest) -> None:
        """Valida una solicitud de operaci√≥n."""
        if not request.operation_type:
            raise ValidationError("Operation type is required")
        
        if not self._is_running:
            raise BrainException("System is not running")
    
    def _update_metrics(self, success: bool, processing_time: float) -> None:
        """Actualiza m√©tricas del sistema."""
        self._metrics["operations_completed"] += 1
        if not success:
            self._metrics["operations_failed"] += 1
        
        self._metrics["total_processing_time_ms"] += processing_time
        self._metrics["avg_response_time_ms"] = (
            self._metrics["total_processing_time_ms"] / self._metrics["operations_completed"]
        )
    
    async def _initialize_component(self, component_name: str) -> Any:
        """Inicializa un componente del sistema."""
        # Implementaci√≥n real cargar√≠a el componente din√°micamente
        # Por ahora retornamos un mock
        class MockComponent:
            async def shutdown(self):
                pass
        
        return MockComponent()
    
    async def _load_plugins(self) -> None:
        """Carga plugins del sistema."""
        pass
    
    async def _start_workers(self) -> None:
        """Inicia workers para procesamiento en background."""
        pass
    
    async def _start_monitoring(self) -> None:
        """Inicia sistema de monitoreo."""
        pass
    
    def _load_config(self, config_path: Optional[str]) -> OrchestratorConfig:
        """Carga configuraci√≥n del orquestador."""
        # Implementaci√≥n real leer√≠a de archivo
        return OrchestratorConfig()
4.2 Archivo: src/indexer/multi_language_parser.py

python
"""
MultiLanguageParser - Parser multi-lenguaje para an√°lisis de c√≥digo.
Soporta Python, JavaScript/TypeScript, Java, C++, Go, Rust mediante tree-sitter.
"""

from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import os
from pathlib import Path
from tree_sitter import Language, Parser, Node
from tree_sitter_languages import get_language, get_parser
from pydantic import BaseModel, Field, validator
import hashlib
import warnings

class LanguageType(Enum):
    """Tipos de lenguaje soportados."""
    PYTHON = "python"
    JAVASCRIPT = "javascript"
    TYPESCRIPT = "typescript"
    JAVA = "java"
    CPP = "cpp"
    GO = "go"
    RUST = "rust"
    CSHARP = "csharp"
    PHP = "php"
    RUBY = "ruby"

class ParseMode(Enum):
    """Modos de parsing."""
    QUICK = "quick"        # Solo estructura b√°sica
    STANDARD = "standard"  # Estructura + imports
    COMPREHENSIVE = "comprehensive"  # Todo + an√°lisis sem√°ntico b√°sico

@dataclass
class ParseResult:
    """Resultado del parsing de un archivo."""
    success: bool
    language: LanguageType
    ast: Optional[Dict] = None
    entities: List[Dict] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    parse_time_ms: float = 0.0
    file_hash: Optional[str] = None

@dataclass
class Entity:
    """Entidad extra√≠da del c√≥digo."""
    type: str  # 'function', 'class', 'variable', 'import', 'comment'
    name: str
    start_line: int
    end_line: int
    start_column: int
    end_column: int
    metadata: Dict[str, Any] = field(default_factory=dict)
    children: List['Entity'] = field(default_factory=list)

class ParserConfig(BaseModel):
    """Configuraci√≥n del parser."""
    enabled_languages: List[LanguageType] = Field(
        default_factory=lambda: list(LanguageType)
    )
    default_mode: ParseMode = ParseMode.STANDARD
    max_file_size_mb: int = 10
    timeout_seconds: int = 30
    include_comments: bool = True
    include_whitespace: bool = False
    cache_parsed_files: bool = True
    cache_size: int = 1000
    
    class Config:
        arbitrary_types_allowed = True

class MultiLanguageParser:
    """
    Parser multi-lenguaje basado en tree-sitter.
    
    Caracter√≠sticas:
    1. Soporte para 10+ lenguajes de programaci√≥n
    2. Extracci√≥n de entidades (funciones, clases, imports)
    3. Validaci√≥n de sintaxis b√°sica
    4. Cach√© de archivos parseados
    5. Tolerante a errores (contin√∫a con otros archivos)
    """
    
    def __init__(self, config: Optional[ParserConfig] = None):
        """
        Inicializa el parser multi-lenguaje.
        
        Args:
            config: Configuraci√≥n del parser (opcional)
        """
        self.config = config or ParserConfig()
        self._parsers: Dict[LanguageType, Parser] = {}
        self._cache: Dict[str, ParseResult] = {}
        self._init_parsers()
        
    def _init_parsers(self) -> None:
        """Inicializa parsers para cada lenguaje."""
        for lang in self.config.enabled_languages:
            try:
                parser = get_parser(lang.value)
                self._parsers[lang] = parser
            except Exception as e:
                warnings.warn(f"Failed to initialize parser for {lang}: {e}")
    
    def parse_file(self, file_path: str, 
                  language: Optional[LanguageType] = None,
                  mode: Optional[ParseMode] = None) -> ParseResult:
        """
        Parsea un archivo de c√≥digo.
        
        Args:
            file_path: Ruta al archivo a parsear
            language: Lenguaje (autodetectado si None)
            mode: Modo de parsing
            
        Returns:
            ParseResult con resultados del parsing
            
        Raises:
            FileNotFoundError: Si el archivo no existe
            ValueError: Si el lenguaje no es soportado
        """
        start_time = time.time()
        
        # Validaciones iniciales
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")
        
        # Verificar tama√±o
        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        if file_size_mb > self.config.max_file_size_mb:
            return ParseResult(
                success=False,
                language=language or LanguageType.PYTHON,
                errors=[f"File too large: {file_size_mb:.2f}MB > {self.config.max_file_size_mb}MB"]
            )
        
        # Calcular hash para cach√©
        file_hash = self._calculate_file_hash(file_path)
        cache_key = f"{file_path}:{file_hash}"
        
        # Verificar cach√©
        if self.config.cache_parsed_files and cache_key in self._cache:
            result = self._cache[cache_key]
            result.metadata["cached"] = True
            return result
        
        # Determinar lenguaje
        if language is None:
            language = self._detect_language(file_path)
        
        if language not in self.config.enabled_languages:
            return ParseResult(
                success=False,
                language=language,
                errors=[f"Language {language.value} not enabled in config"]
            )
        
        # Leer contenido
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            # Intentar con diferentes encodings
            encodings = ['latin-1', 'cp1252', 'iso-8859-1']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                    break
                except UnicodeDecodeError:
                    continue
            else:
                return ParseResult(
                    success=False,
                    language=language,
                    errors=["Failed to decode file with any encoding"]
                )
        
        # Parsear
        parse_mode = mode or self.config.default_mode
        result = self._parse_content(content, language, parse_mode, file_path)
        
        # A√±adir metadata
        result.file_hash = file_hash
        result.metadata.update({
            "file_path": file_path,
            "file_size_bytes": os.path.getsize(file_path),
            "parse_mode": parse_mode.value,
            "encoding_detected": "utf-8"
        })
        
        # Calcular tiempo
        result.parse_time_ms = (time.time() - start_time) * 1000
        
        # Guardar en cach√©
        if self.config.cache_parsed_files and result.success:
            self._cache[cache_key] = result
            # Mantener tama√±o de cach√©
            if len(self._cache) > self.config.cache_size:
                oldest_key = next(iter(self._cache))
                del self._cache[oldest_key]
        
        return result
    
    def parse_directory(self, directory_path: str,
                       language: Optional[LanguageType] = None,
                       mode: Optional[ParseMode] = None,
                       extensions: Optional[List[str]] = None) -> Dict[str, ParseResult]:
        """
        Parsea todos los archivos en un directorio.
        
        Args:
            directory_path: Ruta al directorio
            language: Lenguaje (opcional)
            mode: Modo de parsing (opcional)
            extensions: Extensiones a incluir (None = todas soportadas)
            
        Returns:
            Dict con resultados por archivo
        """
        results = {}
        
        for root, _, files in os.walk(directory_path):
            for file in files:
                file_path = os.path.join(root, file)
                
                # Filtrar por extensi√≥n
                if extensions is not None:
                    ext = os.path.splitext(file)[1].lower()
                    if ext not in extensions:
                        continue
                
                try:
                    result = self.parse_file(file_path, language, mode)
                    results[file_path] = result
                except Exception as e:
                    results[file_path] = ParseResult(
                        success=False,
                        language=language or LanguageType.PYTHON,
                        errors=[str(e)]
                    )
        
        return results
    
    def extract_entities(self, parse_result: ParseResult) -> List[Entity]:
        """
        Extrae entidades del resultado de parsing.
        
        Args:
            parse_result: Resultado del parsing
            
        Returns:
            Lista de entidades extra√≠das
        """
        if not parse_result.success or parse_result.ast is None:
            return []
        
        entities = []
        
        if parse_result.language == LanguageType.PYTHON:
            entities.extend(self._extract_python_entities(parse_result.ast))
        elif parse_result.language == LanguageType.JAVASCRIPT:
            entities.extend(self._extract_javascript_entities(parse_result.ast))
        elif parse_result.language == LanguageType.JAVA:
            entities.extend(self._extract_java_entities(parse_result.ast))
        # ... otros lenguajes
        
        return entities
    
    # M√©todos espec√≠ficos por lenguaje
    
    def _extract_python_entities(self, ast: Dict) -> List[Entity]:
        """Extrae entidades de c√≥digo Python."""
        entities = []
        
        def traverse(node: Dict, parent: Optional[Entity] = None):
            node_type = node.get('type', '')
            
            if node_type == 'function_definition':
                # Extraer funci√≥n
                name_node = next((c for c in node.get('children', []) 
                                if c.get('type') == 'identifier'), None)
                if name_node:
                    func = Entity(
                        type='function',
                        name=name_node.get('text', ''),
                        start_line=node.get('start_line', 0),
                        end_line=node.get('end_line', 0),
                        start_column=node.get('start_column', 0),
                        end_column=node.get('end_column', 0),
                        metadata={
                            'parameters': self._extract_python_parameters(node),
                            'decorators': self._extract_python_decorators(node),
                            'return_annotation': self._extract_return_annotation(node)
                        }
                    )
                    entities.append(func)
                    
                    # Procesar hijos
                    for child in node.get('children', []):
                        traverse(child, func)
            
            elif node_type == 'class_definition':
                # Extraer clase
                name_node = next((c for c in node.get('children', [])
                                if c.get('type') == 'identifier'), None)
                if name_node:
                    class_entity = Entity(
                        type='class',
                        name=name_node.get('text', ''),
                        start_line=node.get('start_line', 0),
                        end_line=node.get('end_line', 0),
                        start_column=node.get('start_column', 0),
                        end_column=node.get('end_column', 0),
                        metadata={
                            'bases': self._extract_python_bases(node),
                            'decorators': self._extract_python_decorators(node)
                        }
                    )
                    entities.append(class_entity)
                    
                    # Procesar hijos
                    for child in node.get('children', []):
                        traverse(child, class_entity)
            
            elif node_type == 'import_statement' or node_type == 'import_from_statement':
                # Extraer import
                import_entity = Entity(
                    type='import',
                    name=self._extract_python_import_name(node),
                    start_line=node.get('start_line', 0),
                    end_line=node.get('end_line', 0),
                    start_column=node.get('start_column', 0),
                    end_column=node.get('end_column', 0),
                    metadata={
                        'module': self._extract_python_import_module(node),
                        'aliases': self._extract_python_import_aliases(node)
                    }
                )
                entities.append(import_entity)
            
            elif node_type == 'assignment':
                # Extraer variable
                var_name = self._extract_python_variable_name(node)
                if var_name:
                    var_entity = Entity(
                        type='variable',
                        name=var_name,
                        start_line=node.get('start_line', 0),
                        end_line=node.get('end_line', 0),
                        start_column=node.get('start_column', 0),
                        end_column=node.get('end_column', 0),
                        metadata={
                            'value_type': self._infer_python_type(node),
                            'is_constant': self._is_python_constant(var_name)
                        }
                    )
                    entities.append(var_entity)
            
            # Recursi√≥n para hijos
            for child in node.get('children', []):
                traverse(child, parent)
        
        traverse(ast)
        return entities
    
    def _extract_python_parameters(self, node: Dict) -> List[Dict]:
        """Extrae par√°metros de una funci√≥n Python."""
        parameters = []
        
        # Buscar node de par√°metros
        params_node = next((c for c in node.get('children', [])
                          if c.get('type') == 'parameters'), None)
        
        if params_node:
            for child in params_node.get('children', []):
                if child.get('type') == 'identifier':
                    parameters.append({
                        'name': child.get('text', ''),
                        'type': 'positional',
                        'default': None
                    })
                elif child.get('type') == 'default_parameter':
                    # Par√°metro con valor por defecto
                    name_node = next((c for c in child.get('children', [])
                                    if c.get('type') == 'identifier'), None)
                    if name_node:
                        parameters.append({
                            'name': name_node.get('text', ''),
                            'type': 'keyword',
                            'default': self._extract_default_value(child)
                        })
        
        return parameters
    
    def _extract_python_decorators(self, node: Dict) -> List[str]:
        """Extrae decoradores de una funci√≥n/clase Python."""
        decorators = []
        
        # Buscar decorators antes de la definici√≥n
        parent = node.get('parent', {})
        if parent:
            for sibling in parent.get('children', []):
                if sibling.get('type') == 'decorator':
                    name = self._extract_decorator_name(sibling)
                    if name:
                        decorators.append(name)
        
        return decorators
    
    def _extract_python_import_name(self, node: Dict) -> str:
        """Extrae el nombre de un import Python."""
        if node.get('type') == 'import_statement':
            # import module
            name_node = next((c for c in node.get('children', [])
                            if c.get('type') == 'dotted_name'), None)
            if name_node:
                return name_node.get('text', '')
        elif node.get('type') == 'import_from_statement':
            # from module import name
            name_node = next((c for c in node.get('children', [])
                            if c.get('type') == 'dotted_name' or 
                            c.get('type') == 'aliased_import'), None)
            if name_node:
                return name_node.get('text', '')
        
        return ''
    
    # M√©todos auxiliares
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """Calcula hash SHA256 de un archivo."""
        hasher = hashlib.sha256()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    def _detect_language(self, file_path: str) -> LanguageType:
        """Detecta lenguaje basado en extensi√≥n del archivo."""
        ext = os.path.splitext(file_path)[1].lower()
        
        extension_map = {
            '.py': LanguageType.PYTHON,
            '.js': LanguageType.JAVASCRIPT,
            '.jsx': LanguageType.JAVASCRIPT,
            '.ts': LanguageType.TYPESCRIPT,
            '.tsx': LanguageType.TYPESCRIPT,
            '.java': LanguageType.JAVA,
            '.cpp': LanguageType.CPP,
            '.cc': LanguageType.CPP,
            '.h': LanguageType.CPP,
            '.hpp': LanguageType.CPP,
            '.go': LanguageType.GO,
            '.rs': LanguageType.RUST,
            '.cs': LanguageType.CSHARP,
            '.php': LanguageType.PHP,
            '.rb': LanguageType.RUBY
        }
        
        return extension_map.get(ext, LanguageType.PYTHON)
    
    def _parse_content(self, content: str, language: LanguageType,
                      mode: ParseMode, file_path: str) -> ParseResult:
        """Parsea contenido de c√≥digo."""
        parser = self._parsers.get(language)
        
        if parser is None:
            return ParseResult(
                success=False,
                language=language,
                errors=[f"Parser not available for {language.value}"]
            )
        
        try:
            tree = parser.parse(bytes(content, 'utf-8'))
            
            # Convertir AST a dict
            ast_dict = self._tree_to_dict(tree.root_node, content)
            
            # Extraer entidades seg√∫n modo
            entities = []
            if mode in [ParseMode.STANDARD, ParseMode.COMPREHENSIVE]:
                entities = self.extract_entities(ParseResult(
                    success=True,
                    language=language,
                    ast=ast_dict
                ))
            
            # Validaciones adicionales para modo comprehensive
            warnings = []
            if mode == ParseMode.COMPREHENSIVE:
                warnings.extend(self._perform_comprehensive_checks(ast_dict, language))
            
            return ParseResult(
                success=True,
                language=language,
                ast=ast_dict,
                entities=[self._entity_to_dict(e) for e in entities],
                warnings=warnings
            )
            
        except Exception as e:
            return ParseResult(
                success=False,
                language=language,
                errors=[f"Parsing error: {str(e)}"]
            )
    
    def _tree_to_dict(self, node: Node, source: bytes) -> Dict:
        """Convierte un nodo tree-sitter a dict."""
        result = {
            'type': node.type,
            'text': node.text.decode('utf-8') if node.text else '',
            'start_line': node.start_point[0] + 1,
            'end_line': node.end_point[0] + 1,
            'start_column': node.start_point[1],
            'end_column': node.end_point[1],
            'children': []
        }
        
        for child in node.children:
            result['children'].append(self._tree_to_dict(child, source))
        
        return result
    
    def _entity_to_dict(self, entity: Entity) -> Dict:
        """Convierte una Entity a dict."""
        return {
            'type': entity.type,
            'name': entity.name,
            'start_line': entity.start_line,
            'end_line': entity.end_line,
            'start_column': entity.start_column,
            'end_column': entity.end_column,
            'metadata': entity.metadata,
            'children': [self._entity_to_dict(c) for c in entity.children]
        }
    
    def _perform_comprehensive_checks(self, ast: Dict, language: LanguageType) -> List[str]:
        """Realiza verificaciones comprehensivas del AST."""
        warnings = []
        
        # Ejemplo: Verificar complejidad ciclom√°tica aproximada
        if language == LanguageType.PYTHON:
            complexity = self._estimate_python_complexity(ast)
            if complexity > 10:
                warnings.append(f"High estimated complexity: {complexity}")
        
        return warnings
    
    def _estimate_python_complexity(self, ast: Dict) -> int:
        """Estima complejidad ciclom√°tica de c√≥digo Python."""
        complexity = 1  # Base
        
        def count_decisions(node: Dict):
            nonlocal complexity
            node_type = node.get('type', '')
            
            # Estructuras que incrementan complejidad
            if node_type in ['if_statement', 'while_statement', 
                           'for_statement', 'elif_clause']:
                complexity += 1
            elif node_type == 'try_statement':
                complexity += 1  # Cada try cuenta
            elif node_type in ['and', 'or']:
                complexity += 1  # Operadores l√≥gicos
            
            for child in node.get('children', []):
                count_decisions(child)
        
        count_decisions(ast)
        return complexity
    
    # M√©todos para otros lenguajes (esqueleto)
    
    def _extract_javascript_entities(self, ast: Dict) -> List[Entity]:
        """Extrae entidades de c√≥digo JavaScript/TypeScript."""
        entities = []
        # Implementaci√≥n espec√≠fica para JavaScript
        return entities
    
    def _extract_java_entities(self, ast: Dict) -> List[Entity]:
        """Extrae entidades de c√≥digo Java."""
        entities = []
        # Implementaci√≥n espec√≠fica para Java
        return entities
    
    # ... m√©todos similares para otros lenguajes

# Ejemplo de uso
if __name__ == "__main__":
    parser = MultiLanguageParser()
    
    # Parsear un archivo
    result = parser.parse_file("example.py", LanguageType.PYTHON)
    
    if result.success:
        print(f"Parsed successfully in {result.parse_time_ms:.2f}ms")
        print(f"Found {len(result.entities)} entities")
        
        for entity in result.entities[:5]:  # Mostrar primeras 5
            print(f"  - {entity['type']}: {entity['name']} "
                  f"(lines {entity['start_line']}-{entity['end_line']})")
    else:
        print(f"Parsing failed: {', '.join(result.errors)}")
4.3 Archivo: src/agents/base_agent.py

python
"""
BaseAgent - Clase base abstracta para todos los agentes de IA.
Define la interfaz com√∫n y comportamiento base para agentes especializados.
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Callable, Union
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import uuid
from datetime import datetime
from pydantic import BaseModel, Field, validator
import json
from ..core.exceptions import AgentException, ValidationError

class AgentState(Enum):
    """Estados posibles de un agente."""
    INITIALIZING = "initializing"
    READY = "ready"
    PROCESSING = "processing"
    LEARNING = "learning"
    ERROR = "error"
    MAINTENANCE = "maintenance"

class AgentCapability(Enum):
    """Capacidades que puede tener un agente."""
    CODE_ANALYSIS = "code_analysis"
    PATTERN_DETECTION = "pattern_detection"
    QUESTION_ANSWERING = "question_answering"
    ARCHITECTURE_REVIEW = "architecture_review"
    SECURITY_ANALYSIS = "security_analysis"
    PERFORMANCE_ANALYSIS = "performance_analysis"
    CODE_GENERATION = "code_generation"
    DOCUMENTATION_GENERATION = "documentation_generation"
    TEST_GENERATION = "test_generation"
    REFACTORING_SUGGESTION = "refactoring_suggestion"

class AgentMemoryType(Enum):
    """Tipos de memoria para agentes."""
    SHORT_TERM = "short_term"      # Memoria inmediata (minutos)
    LONG_TERM = "long_term"        # Memoria persistente (d√≠as/meses)
    EPISODIC = "episodic"          # Memoria de experiencias espec√≠ficas
    SEMANTIC = "semantic"          # Memoria de conceptos generales

@dataclass
class AgentConfig:
    """Configuraci√≥n base para agentes."""
    agent_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    name: str = "BaseAgent"
    version: str = "1.0.0"
    description: str = "Base agent for Project Brain"
    capabilities: List[AgentCapability] = field(default_factory=list)
    max_processing_time: int = 30  # segundos
    confidence_threshold: float = 0.7
    learning_rate: float = 0.1
    memory_size: Dict[AgentMemoryType, int] = field(
        default_factory=lambda: {
            AgentMemoryType.SHORT_TERM: 100,
            AgentMemoryType.LONG_TERM: 1000,
            AgentMemoryType.EPISODIC: 500,
            AgentMemoryType.SEMANTIC: 10000
        }
    )
    dependencies: List[str] = field(default_factory=list)
    enabled: bool = True

class AgentInput(BaseModel):
    """Entrada estandarizada para agentes."""
    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime = Field(default_factory=datetime.now)
    data: Dict[str, Any] = Field(..., description="Datos de entrada")
    context: Optional[Dict[str, Any]] = Field(None, description="Contexto adicional")
    priority: int = Field(1, ge=1, le=10, description="Prioridad (1-10)")
    
    class Config:
        arbitrary_types_allowed = True

class AgentOutput(BaseModel):
    """Salida estandarizada de agentes."""
    request_id: str
    agent_id: str
    success: bool
    data: Optional[Dict[str, Any]] = None
    confidence: float = Field(0.0, ge=0.0, le=1.0)
    reasoning: List[str] = Field(default_factory=list)
    errors: List[str] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)
    processing_time_ms: float = 0.0
    timestamp: datetime = Field(default_factory=datetime.now)
    
    class Config:
        arbitrary_types_allowed = True

class AgentMemory:
    """Sistema de memoria para agentes."""
    
    def __init__(self, config: AgentConfig):
        self.config = config
        self.memories: Dict[AgentMemoryType, List[Dict]] = {
            AgentMemoryType.SHORT_TERM: [],
            AgentMemoryType.LONG_TERM: [],
            AgentMemoryType.EPISODIC: [],
            AgentMemoryType.SEMANTIC: []
        }
    
    def store(self, memory_type: AgentMemoryType, content: Dict) -> str:
        """Almacena un recuerdo en la memoria."""
        memory_id = str(uuid.uuid4())
        memory = {
            "id": memory_id,
            "type": memory_type.value,
            "content": content,
            "timestamp": datetime.now(),
            "access_count": 0
        }
        
        self.memories[memory_type].append(memory)
        
        # Mantener tama√±o m√°ximo
        max_size = self.config.memory_size.get(memory_type, 100)
        if len(self.memories[memory_type]) > max_size:
            self.memories[memory_type].pop(0)
        
        return memory_id
    
    def retrieve(self, memory_type: AgentMemoryType, 
                query: Optional[Dict] = None, 
                limit: int = 10) -> List[Dict]:
        """Recupera recuerdos de la memoria."""
        memories = self.memories[memory_type]
        
        if query is None:
            # Devolver m√°s recientes
            return sorted(memories, key=lambda x: x["timestamp"], reverse=True)[:limit]
        
        # B√∫squeda simple por contenido (implementaci√≥n b√°sica)
        results = []
        for memory in memories:
            if self._matches_query(memory["content"], query):
                results.append(memory)
        
        # Ordenar por relevancia (simplificado)
        return sorted(results, key=lambda x: x["timestamp"], reverse=True)[:limit]
    
    def consolidate(self) -> None:
        """Consolida memorias (mueve de corto a largo plazo)."""
        # Mover memorias viejas de corto a largo plazo
        old_threshold = datetime.now().timestamp() - 3600  # 1 hora
        
        short_term = self.memories[AgentMemoryType.SHORT_TERM]
        long_term = self.memories[AgentMemoryType.LONG_TERM]
        
        to_move = []
        to_keep = []
        
        for memory in short_term:
            if memory["timestamp"].timestamp() < old_threshold:
                to_move.append(memory)
            else:
                to_keep.append(memory)
        
        # Mover
        long_term.extend(to_move)
        self.memories[AgentMemoryType.SHORT_TERM] = to_keep
        
        # Mantener tama√±o
        max_long_term = self.config.memory_size.get(AgentMemoryType.LONG_TERM, 1000)
        if len(long_term) > max_long_term:
            self.memories[AgentMemoryType.LONG_TERM] = long_term[-max_long_term:]
    
    def _matches_query(self, content: Dict, query: Dict) -> bool:
        """Verifica si el contenido coincide con la query."""
        # Implementaci√≥n b√°sica - en agentes reales usar√≠a embeddings
        for key, value in query.items():
            if key in content:
                if isinstance(value, str) and isinstance(content[key], str):
                    if value.lower() in content[key].lower():
                        return True
                elif content[key] == value:
                    return True
        return False

class BaseAgent(ABC):
    """
    Clase base abstracta para todos los agentes de Project Brain.
    
    Todos los agentes deben heredar de esta clase e implementar:
    1. process() - Procesamiento principal
    2. learn() - Aprendizaje de feedback
    3. evaluate() - Auto-evaluaci√≥n
    
    Caracter√≠sticas comunes:
    - Sistema de memoria jer√°rquica
    - Manejo de estado y errores
    - M√©tricas de performance
    - Capacidad de aprendizaje
    - Colaboraci√≥n con otros agentes
    """
    
    def __init__(self, config: AgentConfig):
        """
        Inicializa el agente.
        
        Args:
            config: Configuraci√≥n del agente
            
        Raises:
            ValidationError: Si la configuraci√≥n es inv√°lida
        """
        self.config = config
        self.state = AgentState.INITIALIZING
        self.memory = AgentMemory(config)
        self.metrics: Dict[str, Any] = {
            "requests_processed": 0,
            "success_rate": 1.0,
            "avg_processing_time_ms": 0.0,
            "total_learning_events": 0,
            "confidence_distribution": [],
            "error_types": {}
        }
        self.dependencies: Dict[str, Any] = {}
        self._initialized = False
        
    async def initialize(self, dependencies: Optional[Dict[str, Any]] = None) -> bool:
        """
        Inicializa el agente y sus dependencias.
        
        Args:
            dependencies: Dependencias inyectadas (otros agentes, servicios)
            
        Returns:
            bool: True si la inicializaci√≥n fue exitosa
            
        Raises:
            AgentException: Si hay errores de inicializaci√≥n
        """
        try:
            self.state = AgentState.INITIALIZING
            
            # Verificar dependencias requeridas
            if dependencies:
                self.dependencies = dependencies
                missing_deps = [
                    dep for dep in self.config.dependencies 
                    if dep not in dependencies
                ]
                if missing_deps:
                    raise AgentException(f"Missing dependencies: {missing_deps}")
            
            # Inicializaci√≥n espec√≠fica del agente
            success = await self._initialize_internal()
            
            if success:
                self.state = AgentState.READY
                self._initialized = True
                
                # Consolidar memoria inicial
                self.memory.consolidate()
                
                return True
            else:
                self.state = AgentState.ERROR
                return False
                
        except Exception as e:
            self.state = AgentState.ERROR
            self.metrics["error_types"]["initialization"] = \
                self.metrics["error_types"].get("initialization", 0) + 1
            raise AgentException(f"Failed to initialize agent {self.config.name}: {e}")
    
    @abstractmethod
    async def _initialize_internal(self) -> bool:
        """
        Inicializaci√≥n espec√≠fica del agente.
        Debe ser implementado por cada agente concreto.
        
        Returns:
            bool: True si la inicializaci√≥n fue exitosa
        """
        pass
    
    async def process(self, input_data: AgentInput) -> AgentOutput:
        """
        Procesa una entrada y produce una salida.
        
        Args:
            input_data: Datos de entrada estandarizados
            
        Returns:
            AgentOutput: Resultado del procesamiento
            
        Raises:
            AgentException: Si el agente no est√° inicializado
            ValidationError: Si la entrada es inv√°lida
            TimeoutError: Si excede el tiempo m√°ximo de procesamiento
        """
        if not self._initialized:
            raise AgentException(f"Agent {self.config.name} is not initialized")
        
        if self.state != AgentState.READY:
            raise AgentException(f"Agent {self.config.name} is not ready (state: {self.state})")
        
        start_time = datetime.now()
        self.state = AgentState.PROCESSING
        
        try:
            # Validar entrada
            self._validate_input(input_data)
            
            # Almacenar en memoria de corto plazo
            self.memory.store(
                AgentMemoryType.SHORT_TERM,
                {
                    "type": "input",
                    "data": input_data.dict(),
                    "timestamp": input_data.timestamp
                }
            )
            
            # Procesar (m√©todo abstracto)
            result = await self._process_internal(input_data)
            
            # Validar salida
            self._validate_output(result)
            
            # Calcular m√©tricas
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            self._update_metrics(result, processing_time)
            
            # Almacenar en memoria epis√≥dica
            self.memory.store(
                AgentMemoryType.EPISODIC,
                {
                    "type": "processing",
                    "input": input_data.dict(),
                    "output": result.dict(),
                    "processing_time_ms": processing_time,
                    "success": result.success
                }
            )
            
            # Consolidar memoria peri√≥dicamente
            if self.metrics["requests_processed"] % 10 == 0:
                self.memory.consolidate()
            
            self.state = AgentState.READY
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            # Registrar error
            error_type = type(e).__name__
            self.metrics["error_types"][error_type] = \
                self.metrics["error_types"].get(error_type, 0) + 1
            
            # Crear salida de error
            output = AgentOutput(
                request_id=input_data.request_id,
                agent_id=self.config.agent_id,
                success=False,
                confidence=0.0,
                errors=[str(e)],
                processing_time_ms=processing_time
            )
            
            self.state = AgentState.READY
            return output
    
    @abstractmethod
    async def _process_internal(self, input_data: AgentInput) -> AgentOutput:
        """
        Procesamiento espec√≠fico del agente.
        Debe ser implementado por cada agente concreto.
        
        Args:
            input_data: Datos de entrada
            
        Returns:
            AgentOutput: Resultado del procesamiento
        """
        pass
    
    async def learn(self, feedback: Dict[str, Any]) -> bool:
        """
        Aprende de feedback o experiencias.
        
        Args:
            feedback: Datos de feedback para aprendizaje
            
        Returns:
            bool: True si el aprendizaje fue exitoso
            
        Example feedback:
            {
                "type": "correction",
                "original_input": {...},
                "expected_output": {...},
                "actual_output": {...},
                "confidence_impact": 0.1
            }
        """
        if not self._initialized:
            return False
        
        self.state = AgentState.LEARNING
        
        try:
            # Validar feedback
            if not self._validate_feedback(feedback):
                return False
            
            # Aprendizaje espec√≠fico
            success = await self._learn_internal(feedback)
            
            if success:
                # Almacenar en memoria sem√°ntica
                self.memory.store(
                    AgentMemoryType.SEMANTIC,
                    {
                        "type": "learning",
                        "feedback": feedback,
                        "timestamp": datetime.now(),
                        "agent_version": self.config.version
                    }
                )
                
                self.metrics["total_learning_events"] += 1
                
                # Ajustar confianza si es necesario
                if "confidence_impact" in feedback:
                    self.config.confidence_threshold = max(0.1, min(0.95,
                        self.config.confidence_threshold + feedback["confidence_impact"]
                    ))
            
            self.state = AgentState.READY
            return success
            
        except Exception as e:
            self.state = AgentState.READY
            self.metrics["error_types"]["learning"] = \
                self.metrics["error_types"].get("learning", 0) + 1
            return False
    
    @abstractmethod
    async def _learn_internal(self, feedback: Dict[str, Any]) -> bool:
        """
        Aprendizaje espec√≠fico del agente.
        Debe ser implementado por cada agente concreto.
        
        Args:
            feedback: Datos de feedback
            
        Returns:
            bool: True si el aprendizaje fue exitoso
        """
        pass
    
    async def evaluate(self) -> Dict[str, Any]:
        """
        Eval√∫a el desempe√±o actual del agente.
        
        Returns:
            Dict con m√©tricas de evaluaci√≥n
        """
        evaluation = {
            "agent_id": self.config.agent_id,
            "agent_name": self.config.name,
            "state": self.state.value,
            "initialized": self._initialized,
            "metrics": self.metrics.copy(),
            "memory_stats": {
                mem_type.value: len(self.memory.memories[mem_type])
                for mem_type in AgentMemoryType
            },
            "config": {
                "confidence_threshold": self.config.confidence_threshold,
                "learning_rate": self.config.learning_rate,
                "capabilities": [c.value for c in self.config.capabilities]
            },
            "timestamp": datetime.now()
        }
        
        # Calcular m√©tricas adicionales
        if self.metrics["requests_processed"] > 0:
            evaluation["metrics"]["success_rate"] = (
                1 - (sum(self.metrics["error_types"].values()) / 
                     self.metrics["requests_processed"])
            )
        
        return evaluation
    
    async def get_capabilities(self) -> List[Dict[str, Any]]:
        """
        Obtiene capacidades detalladas del agente.
        
        Returns:
            Lista de capacidades con descripci√≥n
        """
        capabilities = []
        
        for capability in self.config.capabilities:
            cap_info = {
                "name": capability.value,
                "description": self._get_capability_description(capability),
                "confidence_threshold": self.config.confidence_threshold,
                "supported_languages": self._get_supported_languages(capability),
                "examples": self._get_capability_examples(capability)
            }
            capabilities.append(cap_info)
        
        return capabilities
    
    async def shutdown(self) -> bool:
        """
        Apaga el agente de manera controlada.
        
        Returns:
            bool: True si el apagado fue exitoso
        """
        try:
            # Guardar estado si es necesario
            await self._save_state()
            
            # Consolidar memoria final
            self.memory.consolidate()
            
            self.state = AgentState.MAINTENANCE
            self._initialized = False
            
            return True
            
        except Exception as e:
            self.state = AgentState.ERROR
            return False
    
    # M√©todos auxiliares protegidos
    
    def _validate_input(self, input_data: AgentInput) -> None:
        """Valida la entrada del agente."""
        if not input_data.data:
            raise ValidationError("Input data cannot be empty")
        
        # Validaciones espec√≠ficas por agente
        self._validate_input_specific(input_data)
    
    def _validate_output(self, output: AgentOutput) -> None:
        """Valida la salida del agente."""
        if output.confidence < 0.0 or output.confidence > 1.0:
            raise ValidationError(f"Invalid confidence value: {output.confidence}")
        
        if output.success and not output.data:
            raise ValidationError("Successful output must contain data")
    
    def _validate_feedback(self, feedback: Dict) -> bool:
        """Valida datos de feedback."""
        required_fields = ["type", "timestamp"]
        
        for field in required_fields:
            if field not in feedback:
                return False
        
        return True
    
    def _update_metrics(self, output: AgentOutput, processing_time: float) -> None:
        """Actualiza m√©tricas del agente."""
        self.metrics["requests_processed"] += 1
        
        # Actualizar tasa de √©xito
        if output.success:
            current_success = self.metrics["success_rate"]
            new_count = self.metrics["requests_processed"]
            self.metrics["success_rate"] = (
                (current_success * (new_count - 1) + 1) / new_count
            )
        else:
            current_success = self.metrics["success_rate"]
            new_count = self.metrics["requests_processed"]
            self.metrics["success_rate"] = (
                (current_success * (new_count - 1)) / new_count
            )
        
        # Actualizar tiempo promedio de procesamiento
        current_avg = self.metrics["avg_processing_time_ms"]
        new_count = self.metrics["requests_processed"]
        self.metrics["avg_processing_time_ms"] = (
            (current_avg * (new_count - 1) + processing_time) / new_count
        )
        
        # Registrar distribuci√≥n de confianza
        self.metrics["confidence_distribution"].append(output.confidence)
        if len(self.metrics["confidence_distribution"]) > 1000:
            self.metrics["confidence_distribution"].pop(0)
    
    def _get_capability_description(self, capability: AgentCapability) -> str:
        """Obtiene descripci√≥n de una capacidad."""
        descriptions = {
            AgentCapability.CODE_ANALYSIS: "Analyzes code structure, complexity, and patterns",
            AgentCapability.PATTERN_DETECTION: "Detects design patterns and anti-patterns",
            AgentCapability.QUESTION_ANSWERING: "Answers questions about code and projects",
            AgentCapability.ARCHITECTURE_REVIEW: "Reviews and suggests architectural improvements",
            AgentCapability.SECURITY_ANALYSIS: "Analyzes code for security vulnerabilities",
            AgentCapability.PERFORMANCE_ANALYSIS: "Analyzes and suggests performance improvements",
            AgentCapability.CODE_GENERATION: "Generates code based on specifications",
            AgentCapability.DOCUMENTATION_GENERATION: "Generates documentation for code",
            AgentCapability.TEST_GENERATION: "Generates tests for code",
            AgentCapability.REFACTORING_SUGGESTION: "Suggests refactoring opportunities"
        }
        return descriptions.get(capability, "No description available")
    
    def _get_supported_languages(self, capability: AgentCapability) -> List[str]:
        """Obtiene lenguajes soportados para una capacidad."""
        # Por defecto, todos los lenguajes
        # Agentes espec√≠ficos pueden sobrescribir
        return ["python", "javascript", "typescript", "java", "cpp", "go", "rust"]
    
    def _get_capability_examples(self, capability: AgentCapability) -> List[Dict]:
        """Obtiene ejemplos de uso de una capacidad."""
        # Ejemplos gen√©ricos - agentes espec√≠ficos pueden extender
        return []
    
    # M√©todos abstractos para validaci√≥n espec√≠fica
    
    @abstractmethod
    def _validate_input_specific(self, input_data: AgentInput) -> None:
        """Validaci√≥n espec√≠fica de entrada para el agente."""
        pass
    
    @abstractmethod
    async def _save_state(self) -> None:
        """Guarda el estado del agente (para persistencia)."""
        pass

# Ejemplo de agente concreto
class ExampleAgent(BaseAgent):
    """Agente de ejemplo que muestra la implementaci√≥n completa."""
    
    def __init__(self, config: Optional[AgentConfig] = None):
        # Configuraci√≥n espec√≠fica
        if config is None:
            config = AgentConfig(
                name="ExampleAgent",
                description="Example agent for demonstration",
                capabilities=[
                    AgentCapability.CODE_ANALYSIS,
                    AgentCapability.QUESTION_ANSWERING
                ],
                confidence_threshold=0.8,
                learning_rate=0.2
            )
        
        super().__init__(config)
        self.knowledge_base: Dict[str, Any] = {}
    
    async def _initialize_internal(self) -> bool:
        """Inicializaci√≥n espec√≠fica del ExampleAgent."""
        # Cargar conocimiento base, modelos, etc.
        self.knowledge_base = {
            "patterns": self._load_patterns(),
            "rules": self._load_rules(),
            "examples": self._load_examples()
        }
        
        # Verificar que todo se carg√≥ correctamente
        if all(len(v) > 0 for v in self.knowledge_base.values()):
            return True
        
        return False
    
    async def _process_internal(self, input_data: AgentInput) -> AgentOutput:
        """Procesamiento espec√≠fico del ExampleAgent."""
        # Extraer tipo de solicitud
        request_type = input_data.data.get("type", "unknown")
        
        if request_type == "code_analysis":
            return await self._analyze_code(input_data)
        elif request_type == "question":
            return await self._answer_question(input_data)
        else:
            return AgentOutput(
                request_id=input_data.request_id,
                agent_id=self.config.agent_id,
                success=False,
                confidence=0.0,
                errors=[f"Unknown request type: {request_type}"]
            )
    
    async def _learn_internal(self, feedback: Dict[str, Any]) -> bool:
        """Aprendizaje espec√≠fico del ExampleAgent."""
        feedback_type = feedback.get("type")
        
        if feedback_type == "correction":
            # Aprender de correcci√≥n
            return await self._learn_from_correction(feedback)
        elif feedback_type == "reinforcement":
            # Reforzar conocimiento existente
            return await self._reinforce_knowledge(feedback)
        else:
            return False
    
    def _validate_input_specific(self, input_data: AgentInput) -> None:
        """Validaci√≥n espec√≠fica para ExampleAgent."""
        if "type" not in input_data.data:
            raise ValidationError("Input must contain 'type' field")
        
        valid_types = ["code_analysis", "question"]
        if input_data.data["type"] not in valid_types:
            raise ValidationError(f"Type must be one of {valid_types}")
    
    async def _save_state(self) -> None:
        """Guarda el estado del ExampleAgent."""
        # En una implementaci√≥n real, esto guardar√≠a a disco/DB
        pass
    
    # M√©todos espec√≠ficos de implementaci√≥n
    
    async def _analyze_code(self, input_data: AgentInput) -> AgentOutput:
        """Analiza c√≥digo (implementaci√≥n de ejemplo)."""
        code = input_data.data.get("code", "")
        
        # An√°lisis simple
        issues = self._find_issues(code)
        metrics = self._calculate_metrics(code)
        
        confidence = 0.9 if len(code) > 0 else 0.0
        
        return AgentOutput(
            request_id=input_data.request_id,
            agent_id=self.config.agent_id,
            success=True,
            data={
                "issues": issues,
                "metrics": metrics,
                "suggestions": self._generate_suggestions(issues, metrics)
            },
            confidence=confidence,
            reasoning=["Analyzed code structure", "Checked for common patterns"],
            warnings=["This is example analysis only"] if confidence < 0.5 else []
        )
    
    async def _answer_question(self, input_data: AgentInput) -> AgentOutput:
        """Responde pregunta (implementaci√≥n de ejemplo)."""
        question = input_data.data.get("question", "")
        
        # Respuesta simple basada en conocimiento
        answer = self._find_answer(question)
        confidence = self._calculate_answer_confidence(question, answer)
        
        return AgentOutput(
            request_id=input_data.request_id,
            agent_id=self.config.agent_id,
            success=True,
            data={
                "answer": answer,
                "sources": self._find_sources(question),
                "related_questions": self._suggest_related_questions(question)
            },
            confidence=confidence,
            reasoning=["Parsed question", "Searched knowledge base", "Formulated answer"]
        )
    
    async def _learn_from_correction(self, feedback: Dict) -> bool:
        """Aprende de correcci√≥n (implementaci√≥n de ejemplo)."""
        # Actualizar conocimiento base con correcci√≥n
        correction = feedback.get("correction", {})
        topic = correction.get("topic")
        
        if topic and topic in self.knowledge_base:
            self.knowledge_base[topic].append(correction)
            return True
        
        return False
    
    async def _reinforce_knowledge(self, feedback: Dict) -> bool:
        """Refuerza conocimiento (implementaci√≥n de ejemplo)."""
        # Incrementar peso de conocimiento confirmado
        confirmed_knowledge = feedback.get("knowledge", {})
        
        for key, value in confirmed_knowledge.items():
            if key in self.knowledge_base:
                # En implementaci√≥n real, incrementar√≠a pesos/confianza
                pass
        
        return True
    
    # M√©todos auxiliares (implementaci√≥n simplificada)
    
    def _load_patterns(self) -> List[Dict]:
        return [{"name": "example_pattern", "description": "Example pattern"}]
    
    def _load_rules(self) -> List[Dict]:
        return [{"rule": "example_rule", "condition": "always"}]
    
    def _load_examples(self) -> List[Dict]:
        return [{"input": "example", "output": "result"}]
    
    def _find_issues(self, code: str) -> List[Dict]:
        return [{"type": "example", "line": 1, "description": "Example issue"}]
    
    def _calculate_metrics(self, code: str) -> Dict[str, Any]:
        return {"lines": len(code.split('\n')), "complexity": 1}
    
    def _generate_suggestions(self, issues: List[Dict], metrics: Dict) -> List[str]:
        return ["Example suggestion"]
    
    def _find_answer(self, question: str) -> str:
        return "This is an example answer based on the knowledge base."
    
    def _calculate_answer_confidence(self, question: str, answer: str) -> float:
        return 0.8 if question else 0.0
    
    def _find_sources(self, question: str) -> List[Dict]:
        return [{"type": "example", "content": "Example source"}]
    
    def _suggest_related_questions(self, question: str) -> List[str]:
        return ["Related example question 1", "Related example question 2"]

# Ejemplo de uso
if __name__ == "__main__":
    async def main():
        # Crear y configurar agente
        config = AgentConfig(
            name="TestAgent",
            capabilities=[AgentCapability.CODE_ANALYSIS],
            confidence_threshold=0.7
        )
        
        agent = ExampleAgent(config)
        
        # Inicializar
        success = await agent.initialize()
        print(f"Agent initialized: {success}")
        
        # Procesar entrada
        input_data = AgentInput(
            data={
                "type": "code_analysis",
                "code": "def example():\n    return 42"
            }
        )
        
        output = await agent.process(input_data)
        print(f"Output success: {output.success}")
        print(f"Confidence: {output.confidence}")
        
        if output.success and output.data:
            print(f"Issues found: {len(output.data.get('issues', []))}")
        
        # Evaluar agente
        evaluation = await agent.evaluate()
        print(f"Requests processed: {evaluation['metrics']['requests_processed']}")
        
        # Apagar
        await agent.shutdown()
    
    asyncio.run(main())
5. FLUJOS DE TRABAJO DETALLADOS

5.1 Flujo Completo: An√°lisis de Proyecto desde Cero

python
"""
Flujo: analyze_project_workflow
Descripci√≥n: An√°lisis completo de un proyecto desde cero.
Tiempo estimado: 2-30 minutos dependiendo del tama√±o.
"""

async def analyze_project_workflow(project_path: str, options: Dict) -> Dict[str, Any]:
    """
    Flujo completo para an√°lisis de proyecto.
    
    Pasos:
    1. Validaci√≥n y preparaci√≥n
    2. Escaneo de estructura
    3. Parsing multi-lenguaje
    4. Extracci√≥n de entidades
    5. Construcci√≥n de grafo de conocimiento
    6. Generaci√≥n de embeddings
    7. An√°lisis de calidad
    8. Generaci√≥n de reporte
    9. Integraci√≥n con sistema de aprendizaje
    
    Returns:
        Dict con resultados completos del an√°lisis
    """
    
    workflow_steps = [
        {
            "name": "validate_project",
            "function": validate_project_structure,
            "description": "Validar estructura y permisos del proyecto",
            "timeout": 60,
            "retries": 3
        },
        {
            "name": "scan_files",
            "function": scan_project_files,
            "description": "Escaneo recursivo de archivos",
            "dependencies": ["validate_project"],
            "parallelizable": True
        },
        {
            "name": "parse_code",
            "function": parse_all_source_files,
            "description": "Parsing multi-lenguaje de c√≥digo",
            "dependencies": ["scan_files"],
            "parallelizable": True,
            "batch_size": 50
        },
        {
            "name": "extract_entities",
            "function": extract_code_entities,
            "description": "Extracci√≥n de funciones, clases, imports",
            "dependencies": ["parse_code"]
        },
        {
            "name": "build_dependency_graph",
            "function": build_dependency_graph,
            "description": "Construcci√≥n de grafo de dependencias",
            "dependencies": ["extract_entities"]
        },
        {
            "name": "generate_embeddings",
            "function": generate_code_embeddings,
            "description": "Generaci√≥n de embeddings vectoriales",
            "dependencies": ["extract_entities"],
            "resource_intensive": True
        },
        {
            "name": "analyze_quality",
            "function": perform_quality_analysis,
            "description": "An√°lisis de calidad y m√©tricas",
            "dependencies": ["extract_entities", "build_dependency_graph"]
        },
        {
            "name": "detect_patterns",
            "function": detect_design_patterns,
            "description": "Detecci√≥n de patrones de dise√±o",
            "dependencies": ["extract_entities", "build_dependency_graph"]
        },
        {
            "name": "assess_security",
            "function": perform_security_assessment,
            "description": "Evaluaci√≥n de seguridad",
            "dependencies": ["parse_code"]
        },
        {
            "name": "generate_report",
            "function": compile_analysis_report,
            "description": "Compilaci√≥n de reporte final",
            "dependencies": ["analyze_quality", "detect_patterns", "assess_security"]
        },
        {
            "name": "integrate_knowledge",
            "function": integrate_with_knowledge_base,
            "description": "Integraci√≥n con base de conocimiento",
            "dependencies": ["generate_embeddings", "build_dependency_graph"]
        }
    ]
    
    # Ejecuci√≥n del flujo con manejo de errores
    context = {
        "project_path": project_path,
        "options": options,
        "start_time": datetime.now(),
        "results": {},
        "errors": [],
        "warnings": []
    }
    
    for step in workflow_steps:
        step_name = step["name"]
        
        try:
            # Verificar dependencias
            deps = step.get("dependencies", [])
            for dep in deps:
                if dep not in context["results"]:
                    raise DependencyError(f"Dependency {dep} not completed")
            
            # Ejecutar paso
            start = time.time()
            result = await step["function"](context)
            elapsed = time.time() - start
            
            context["results"][step_name] = {
                "success": True,
                "result": result,
                "time_seconds": elapsed,
                "timestamp": datetime.now()
            }
            
            # Publicar progreso
            await publish_progress(step_name, elapsed, result)
            
        except Exception as e:
            context["results"][step_name] = {
                "success": False,
                "error": str(e),
                "time_seconds": 0,
                "timestamp": datetime.now()
            }
            context["errors"].append(f"{step_name}: {e}")
            
            # Si es paso cr√≠tico, abortar
            if step_name in ["validate_project", "scan_files"]:
                raise WorkflowError(f"Critical step {step_name} failed: {e}")
    
    # Compilar resultados finales
    final_result = compile_final_results(context)
    
    # Aprender del an√°lisis (si est√° habilitado)
    if options.get("enable_learning", True):
        await learn_from_analysis(final_result)
    
    return final_result

def compile_final_results(context: Dict) -> Dict[str, Any]:
    """Compila resultados finales del an√°lisis."""
    return {
        "project_id": context.get("project_id", generate_project_id()),
        "analysis_id": str(uuid.uuid4()),
        "status": "completed",
        "total_time_seconds": (datetime.now() - context["start_time"]).total_seconds(),
        "summary": {
            "files_analyzed": count_files_analyzed(context),
            "entities_extracted": count_entities(context),
            "issues_found": count_issues(context),
            "patterns_detected": count_patterns(context),
            "security_issues": count_security_issues(context)
        },
        "detailed_results": {
            step: result for step, result in context["results"].items()
        },
        "recommendations": generate_recommendations(context),
        "next_steps": suggest_next_steps(context),
        "metadata": {
            "timestamp": datetime.now(),
            "version": "1.0.0",
            "options_used": context["options"]
        }
    }
5.2 Flujo Completo: Procesamiento de Pregunta del Usuario

python
"""
Flujo: process_question_workflow
Descripci√≥n: Procesamiento completo de pregunta desde entrada a respuesta.
Tiempo estimado: < 5 segundos para la mayor√≠a de preguntas.
"""

async def process_question_workflow(question: str, context: Dict) -> Dict[str, Any]:
    """
    Flujo completo para procesamiento de preguntas.
    
    Pasos:
    1. Preprocesamiento y validaci√≥n
    2. Clasificaci√≥n de intenci√≥n
    3. Extracci√≥n de entidades
    4. B√∫squeda de contexto relevante
    5. Enrutamiento a agentes especializados
    6. S√≠ntesis de respuestas
    7. Aprendizaje de la interacci√≥n
    
    Returns:
        Dict con respuesta estructurada
    """
    
    workflow = {
        "preprocessing": {
            "function": preprocess_question,
            "description": "Limpieza y validaci√≥n de pregunta",
            "timeout": 1,
            "inputs": ["question", "context"],
            "outputs": ["cleaned_question", "language", "complexity"]
        },
        "intent_classification": {
            "function": classify_intent,
            "description": "Clasificaci√≥n de intenci√≥n de la pregunta",
            "dependencies": ["preprocessing"],
            "inputs": ["cleaned_question", "context"],
            "outputs": ["intent", "confidence", "sub_intents"]
        },
        "entity_extraction": {
            "function": extract_entities,
            "description": "Extracci√≥n de entidades mencionadas",
            "dependencies": ["preprocessing"],
            "inputs": ["cleaned_question", "context"],
            "outputs": ["entities", "entity_types", "entity_confidence"]
        },
        "context_retrieval": {
            "function": retrieve_relevant_context,
            "description": "B√∫squeda de contexto relevante",
            "dependencies": ["intent_classification", "entity_extraction"],
            "inputs": ["intent", "entities", "context"],
            "outputs": ["context_documents", "relevance_scores", "source_types"]
        },
        "agent_routing": {
            "function": route_to_agents,
            "description": "Enrutamiento a agentes especializados",
            "dependencies": ["intent_classification", "context_retrieval"],
            "inputs": ["intent", "context_documents", "entities"],
            "outputs": ["selected_agents", "routing_confidence", "expected_outputs"]
        },
        "agent_processing": {
            "function": process_with_agents,
            "description": "Procesamiento paralelo por agentes",
            "dependencies": ["agent_routing"],
            "parallelizable": True,
            "inputs": ["selected_agents", "context_documents", "question"],
            "outputs": ["agent_responses", "agent_confidences", "processing_times"]
        },
        "response_synthesis": {
            "function": synthesize_response,
            "description": "S√≠ntesis de respuesta coherente",
            "dependencies": ["agent_processing"],
            "inputs": ["agent_responses", "context_documents", "intent"],
            "outputs": ["final_answer", "sources", "reasoning_chain"]
        },
        "confidence_calculation": {
            "function": calculate_overall_confidence,
            "description": "C√°lculo de confianza final",
            "dependencies": ["agent_processing", "response_synthesis"],
            "inputs": ["agent_confidences", "relevance_scores", "entity_confidence"],
            "outputs": ["overall_confidence", "confidence_breakdown"]
        },
        "formatting": {
            "function": format_response,
            "description": "Formateo para presentaci√≥n",
            "dependencies": ["response_synthesis", "confidence_calculation"],
            "inputs": ["final_answer", "sources", "overall_confidence"],
            "outputs": ["formatted_response", "suggested_followups", "alternative_formats"]
        },
        "learning": {
            "function": learn_from_interaction,
            "description": "Aprendizaje de la interacci√≥n",
            "dependencies": ["formatting"],
            "async": True,  # Se ejecuta en background
            "inputs": ["question", "formatted_response", "context", "confidence_breakdown"],
            "outputs": ["learning_updates", "knowledge_refinements"]
        }
    }
    
    # Estado de ejecuci√≥n
    state = {
        "question": question,
        "context": context,
        "start_time": datetime.now(),
        "step_results": {},
        "final_output": None
    }
    
    # Ejecutar pasos en orden (con manejo de dependencias)
    for step_name, step_config in workflow.items():
        try:
            # Verificar si podemos ejecutar este paso
            if not can_execute_step(step_name, step_config, state):
                continue
            
            # Ejecutar paso
            result = await execute_step(step_name, step_config, state)
            state["step_results"][step_name] = result
            
            # Actualizar estado con outputs
            for output_key, output_value in zip(
                step_config.get("outputs", []), 
                result.get("outputs", [])
            ):
                state[output_key] = output_value
            
            # Si es paso de aprendizaje as√≠ncrono, continuar sin esperar
            if step_config.get("async", False):
                asyncio.create_task(
                    step_config["function"](*[state[inp] for inp in step_config["inputs"]])
                )
                continue
            
        except Exception as e:
            # Manejar error seg√∫n criticidad del paso
            if step_name in ["preprocessing", "intent_classification"]:
                raise QuestionProcessingError(f"Critical step {step_name} failed: {e}")
            
            # Para pasos no cr√≠ticos, continuar con valores por defecto
            state["step_results"][step_name] = {
                "success": False,
                "error": str(e),
                "outputs": get_default_outputs(step_config)
            }
    
    # Compilar respuesta final
    final_response = compile_question_response(state)
    state["final_output"] = final_response
    
    return final_response

def compile_question_response(state: Dict) -> Dict[str, Any]:
    """Compila respuesta final para pregunta."""
    return {
        "answer": {
            "text": state.get("formatted_response", "No response generated"),
            "structured": extract_structured_answer(state),
            "sources": state.get("sources", []),
            "reasoning": state.get("reasoning_chain", []),
            "confidence_breakdown": state.get("confidence_breakdown", {})
        },
        "metadata": {
            "question": state["question"],
            "processing_time_ms": (datetime.now() - state["start_time"]).total_seconds() * 1000,
            "steps_executed": list(state["step_results"].keys()),
            "success_rate": calculate_success_rate(state),
            "agents_consulted": extract_agents_consulted(state),
            "timestamp": datetime.now()
        },
        "context_updates": {
            "new_entities": extract_new_entities(state),
            "conversation_history": update_conversation_history(state),
            "learning_applied": state.get("learning_updates", False)
        },
        "suggestions": {
            "followup_questions": state.get("suggested_followups", []),
            "related_topics": suggest_related_topics(state),
            "alternative_formats": state.get("alternative_formats", {})
        }
    }
6. MODELOS DE DATOS COMPLETOS

6.1 Esquema de Base de Conocimiento Principal

python
"""
Esquemas Pydantic para todos los modelos de datos del sistema.
"""

from typing import Dict, List, Optional, Any, Union
from datetime import datetime
from enum import Enum
from pydantic import BaseModel, Field, validator
import uuid

class EntityType(str, Enum):
    """Tipos de entidades en el sistema."""
    FILE = "file"
    FUNCTION = "function"
    CLASS = "class"
    VARIABLE = "variable"
    MODULE = "module"
    PACKAGE = "package"
    INTERFACE = "interface"
    ENUM = "enum"
    STRUCT = "struct"
    TRAIT = "trait"
    CONCEPT = "concept"
    PATTERN = "pattern"
    ISSUE = "issue"
    RECOMMENDATION = "recommendation"

class RelationshipType(str, Enum):
    """Tipos de relaciones entre entidades."""
    CONTAINS = "contains"
    CALLS = "calls"
    IMPORTS = "imports"
    INHERITS = "inherits"
    IMPLEMENTS = "implements"
    USES = "uses"
    DEPENDS_ON = "depends_on"
    REFERENCES = "references"
    SIMILAR_TO = "similar_to"
    PART_OF = "part_of"
    CREATES = "creates"
    MODIFIES = "modifies"
    TESTS = "tests"
    DOCUMENTS = "documents"

class Language(str, Enum):
    """Lenguajes de programaci√≥n soportados."""
    PYTHON = "python"
    JAVASCRIPT = "javascript"
    TYPESCRIPT = "typescript"
    JAVA = "java"
    CPP = "cpp"
    GO = "go"
    RUST = "rust"
    CSHARP = "csharp"
    PHP = "php"
    RUBY = "ruby"
    SWIFT = "swift"
    KOTLIN = "kotlin"
    SCALA = "scala"

class ComplexityLevel(str, Enum):
    """Niveles de complejidad."""
    TRIVIAL = "trivial"
    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"
    VERY_COMPLEX = "very_complex"

class IssueSeverity(str, Enum):
    """Severidad de issues encontrados."""
    INFO = "info"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class BaseEntity(BaseModel):
    """Entidad base con propiedades comunes."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    type: EntityType
    name: str
    description: Optional[str] = None
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    class Config:
        arbitrary_types_allowed = True

class FileEntity(BaseEntity):
    """Entidad que representa un archivo."""
    type: EntityType = EntityType.FILE
    path: str
    name: str
    extension: str
    language: Language
    size_bytes: int
    line_count: int
    content_hash: str
    encoding: str = "utf-8"
    
    # M√©tricas
    complexity: Optional[ComplexityLevel] = None
    maintainability_index: Optional[float] = Field(None, ge=0.0, le=100.0)
    test_coverage: Optional[float] = Field(None, ge=0.0, le=100.0)
    
    # An√°lisis
    issues: List['IssueEntity'] = Field(default_factory=list)
    functions: List['FunctionEntity'] = Field(default_factory=list)
    classes: List['ClassEntity'] = Field(default_factory=list)
    imports: List['ImportEntity'] = Field(default_factory=list)
    
    @validator('path')
    def validate_path(cls, v):
        if not v or v.strip() == "":
            raise ValueError("Path cannot be empty")
        return v
    
    @validator('line_count')
    def validate_line_count(cls, v):
        if v < 0:
            raise ValueError("Line count cannot be negative")
        return v

class FunctionEntity(BaseEntity):
    """Entidad que representa una funci√≥n/m√©todo."""
    type: EntityType = EntityType.FUNCTION
    name: str
    signature: str
    return_type: Optional[str] = None
    parameters: List['Parameter'] = Field(default_factory=list)
    start_line: int
    end_line: int
    start_column: int
    end_column: int
    
    # Propiedades espec√≠ficas
    is_async: bool = False
    is_generator: bool = False
    is_coroutine: bool = False
    decorators: List[str] = Field(default_factory=list)
    docstring: Optional[str] = None
    
    # M√©tricas
    cyclomatic_complexity: int = Field(1, ge=1)
    cognitive_complexity: int = Field(0, ge=0)
    lines_of_code: int = Field(0, ge=0)
    parameter_count: int = Field(0, ge=0)
    
    # An√°lisis
    calls: List['FunctionCall'] = Field(default_factory=list)
    variables: List['VariableEntity'] = Field(default_factory=list)
    issues: List['IssueEntity'] = Field(default_factory=list)
    
    @validator('end_line')
    def validate_end_line(cls, v, values):
        if 'start_line' in values and v < values['start_line']:
            raise ValueError("End line must be greater than or equal to start line")
        return v
    
    @validator('cyclomatic_complexity')
    def validate_cyclomatic_complexity(cls, v):
        if v > 50:
            warnings.warn(f"Very high cyclomatic complexity: {v}")
        return v

class ClassEntity(BaseEntity):
    """Entidad que representa una clase."""
    type: EntityType = EntityType.CLASS
    name: str
    full_name: Optional[str] = None
    bases: List[str] = Field(default_factory=list)
    decorators: List[str] = Field(default_factory=list)
    docstring: Optional[str] = None
    
    # Miembros
    methods: List[FunctionEntity] = Field(default_factory=list)
    attributes: List['ClassAttribute'] = Field(default_factory=list)
    properties: List['Property'] = Field(default_factory=list)
    
    # Caracter√≠sticas
    is_abstract: bool = False
    is_final: bool = False
    is_dataclass: bool = False
    access_modifier: str = "public"  # public, protected, private
    
    # M√©tricas
    method_count: int = Field(0, ge=0)
    attribute_count: int = Field(0, ge=0)
    inheritance_depth: int = Field(0, ge=0)
    cohesion_score: Optional[float] = Field(None, ge=0.0, le=1.0)
    
    # An√°lisis
    issues: List['IssueEntity'] = Field(default_factory=list)
    design_patterns: List['PatternEntity'] = Field(default_factory=list)

class VariableEntity(BaseEntity):
    """Entidad que representa una variable."""
    type: EntityType = EntityType.VARIABLE
    name: str
    var_type: Optional[str] = None  # Tipo inferido o declarado
    value: Optional[str] = None  # Valor inicial (si se puede determinar)
    scope: str  # local, class, global, builtin
    is_constant: bool = False
    is_mutable: bool = True
    line_declared: int
    usages: List[int] = Field(default_factory=list)  # L√≠neas donde se usa
    
    # An√°lisis
    type_inference_confidence: Optional[float] = Field(None, ge=0.0, le=1.0)
    issues: List['IssueEntity'] = Field(default_factory=list)

class ImportEntity(BaseEntity):
    """Entidad que representa un import/require/include."""
    type: EntityType = EntityType.MODULE
    module: str
    imports: List[str] = Field(default_factory=list)  # Qu√© importa espec√≠ficamente
    alias: Optional[str] = None
    is_relative: bool = False
    level: int = Field(0, ge=0)  # Para imports relativos en Python
    
    # Metadata
    is_used: bool = False
    usage_count: int = 0
    line_number: int

class IssueEntity(BaseEntity):
    """Entidad que representa un issue encontrado en el c√≥digo."""
    type: EntityType = EntityType.ISSUE
    issue_type: str  # bug, vulnerability, code_smell, performance, etc.
    severity: IssueSeverity
    message: str
    location: 'CodeLocation'
    
    # Detalles
    rule_id: Optional[str] = None
    rule_category: Optional[str] = None
    effort_minutes: Optional[int] = Field(None, ge=0)
    tags: List[str] = Field(default_factory=list)
    
    # An√°lisis
    confidence: float = Field(0.8, ge=0.0, le=1.0)
    false_positive: bool = False
    fixed: bool = False
    
    # Sugerencias
    suggestion: Optional[str] = None
    example_fix: Optional[str] = None
    references: List[str] = Field(default_factory=list)

class PatternEntity(BaseEntity):
    """Entidad que representa un patr√≥n de dise√±o detectado."""
    type: EntityType = EntityType.PATTERN
    pattern_type: str  # Singleton, Factory, Observer, etc.
    category: str  # Creational, Structural, Behavioral, etc.
    confidence: float = Field(0.0, ge=0.0, le=1.0)
    
    # Elementos involucrados
    participants: List[Dict[str, Any]] = Field(default_factory=list)
    relationships: List[Dict[str, Any]] = Field(default_factory=list)
    
    # An√°lisis
    implementation_quality: Optional[float] = Field(None, ge=0.0, le=1.0)
    benefits: List[str] = Field(default_factory=list)
    drawbacks: List[str] = Field(default_factory=list)
    
    # Metadata
    is_anti_pattern: bool = False
    recommendation: Optional[str] = None

class Relationship(BaseModel):
    """Relaci√≥n entre dos entidades."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    type: RelationshipType
    source_id: str
    target_id: str
    
    # Propiedades de la relaci√≥n
    weight: float = Field(1.0, ge=0.0, le=1.0)
    confidence: float = Field(1.0, ge=0.0, le=1.0)
    
    # Metadata
    created_at: datetime = Field(default_factory=datetime.now)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    @validator('source_id')
    def validate_source_id(cls, v):
        if not v or v.strip() == "":
            raise ValueError("Source ID cannot be empty")
        return v
    
    @validator('target_id')
    def validate_target_id(cls, v):
        if not v or v.strip() == "":
            raise ValueError("Target ID cannot be empty")
        return v

# Modelos auxiliares

class Parameter(BaseModel):
    """Par√°metro de una funci√≥n."""
    name: str
    param_type: Optional[str] = None
    default_value: Optional[str] = None
    is_optional: bool = False
    is_varargs: bool = False
    is_kwargs: bool = False
    
    class Config:
        arbitrary_types_allowed = True

class FunctionCall(BaseModel):
    """Llamada a funci√≥n encontrada en el c√≥digo."""
    callee_name: str
    callee_signature: Optional[str] = None
    line_number: int
    arguments: List[str] = Field(default_factory=list)
    is_direct: bool = True
    
    class Config:
        arbitrary_types_allowed = True

class ClassAttribute(BaseModel):
    """Atributo de una clase."""
    name: str
    attr_type: Optional[str] = None
    default_value: Optional[str] = None
    access_modifier: str = "public"
    is_class_var: bool = False
    is_property: bool = False
    line_number: int
    
    class Config:
        arbitrary_types_allowed = True

class Property(BaseModel):
    """Propiedad de una clase (getter/setter)."""
    name: str
    getter: Optional[FunctionEntity] = None
    setter: Optional[FunctionEntity] = None
    deleter: Optional[FunctionEntity] = None
    docstring: Optional[str] = None
    line_number: int
    
    class Config:
        arbitrary_types_allowed = True

class CodeLocation(BaseModel):
    """Ubicaci√≥n en el c√≥digo fuente."""
    file_path: str
    start_line: int
    end_line: int
    start_column: Optional[int] = None
    end_column: Optional[int] = None
    code_snippet: Optional[str] = None
    
    class Config:
        arbitrary_types_allowed = True

# Actualizar referencias forward
FunctionEntity.update_forward_refs()
ClassEntity.update_forward_refs()
IssueEntity.update_forward_refs()

class KnowledgeGraph(BaseModel):
    """Grafo completo de conocimiento de un proyecto."""
    project_id: str
    entities: Dict[str, BaseEntity] = Field(default_factory=dict)
    relationships: List[Relationship] = Field(default_factory=list)
    
    # M√©tricas del grafo
    entity_count: int = 0
    relationship_count: int = 0
    density: Optional[float] = None
    diameter: Optional[int] = None
    average_degree: Optional[float] = None
    
    # Metadata
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    analysis_version: str = "1.0.0"
    
    class Config:
        arbitrary_types_allowed = True
    
    def add_entity(self, entity: BaseEntity) -> None:
        """A√±ade una entidad al grafo."""
        self.entities[entity.id] = entity
        self.entity_count = len(self.entities)
    
    def add_relationship(self, relationship: Relationship) -> None:
        """A√±ade una relaci√≥n al grafo."""
        # Verificar que las entidades existan
        if (relationship.source_id not in self.entities or 
            relationship.target_id not in self.entities):
            raise ValueError("Source or target entity does not exist in graph")
        
        self.relationships.append(relationship)
        self.relationship_count = len(self.relationships)
    
    def get_entity(self, entity_id: str) -> Optional[BaseEntity]:
        """Obtiene una entidad por ID."""
        return self.entities.get(entity_id)
    
    def find_entities(self, 
                     entity_type: Optional[EntityType] = None,
                     **filters) -> List[BaseEntity]:
        """Encuentra entidades que coincidan con los criterios."""
        results = []
        
        for entity in self.entities.values():
            # Filtrar por tipo
            if entity_type and entity.type != entity_type:
                continue
            
            # Filtrar por otros criterios
            matches = True
            for key, value in filters.items():
                if not hasattr(entity, key) or getattr(entity, key) != value:
                    matches = False
                    break
            
            if matches:
                results.append(entity)
        
        return results
    
    def get_relationships(self, 
                         source_id: Optional[str] = None,
                         target_id: Optional[str] = None,
                         relationship_type: Optional[RelationshipType] = None) -> List[Relationship]:
        """Obtiene relaciones que coincidan con los criterios."""
        results = []
        
        for rel in self.relationships:
            if source_id and rel.source_id != source_id:
                continue
            if target_id and rel.target_id != target_id:
                continue
            if relationship_type and rel.type != relationship_type:
                continue
            
            results.append(rel)
        
        return results
    
    def calculate_metrics(self) -> None:
        """Calcula m√©tricas del grafo."""
        if self.entity_count == 0:
            return
        
        # Calcular densidad (para grafos no dirigidos)
        max_possible_edges = self.entity_count * (self.entity_count - 1) / 2
        if max_possible_edges > 0:
            self.density = self.relationship_count / max_possible_edges
        
        # Calcular grado promedio
        degree_sum = 0
        for entity_id in self.entities:
            out_degree = len(self.get_relationships(source_id=entity_id))
            in_degree = len(self.get_relationships(target_id=entity_id))
            degree_sum += out_degree + in_degree
        
        self.average_degree = degree_sum / self.entity_count if self.entity_count > 0 else 0
    
    def to_networkx(self):
        """Convierte a grafo de NetworkX para an√°lisis."""
        import networkx as nx
        
        G = nx.DiGraph()
        
        # A√±adir nodos
        for entity_id, entity in self.entities.items():
            G.add_node(entity_id, **entity.dict())
        
        # A√±adir aristas
        for rel in self.relationships:
            G.add_edge(rel.source_id, rel.target_id, 
                      type=rel.type, weight=rel.weight)
        
        return G
    
    def export(self, format: str = "json") -> Union[Dict, str]:
        """Exporta el grafo en diferentes formatos."""
        if format == "json":
            return self.dict()
        elif format == "graphml":
            return self._to_graphml()
        elif format == "cypher":
            return self._to_cypher()
        else:
            raise ValueError(f"Unsupported format: {format}")
    
    def _to_graphml(self) -> str:
        """Exporta a formato GraphML."""
        # Implementaci√≥n simplificada
        xml_lines = [
            '<?xml version="1.0" encoding="UTF-8"?>',
            '<graphml xmlns="http://graphml.graphdrawing.org/xmlns">',
            '  <key id="type" for="node" attr.name="type" attr.type="string"/>',
            '  <key id="name" for="node" attr.name="name" attr.type="string"/>',
            '  <graph id="G" edgedefault="directed">'
        ]
        
        # Nodos
        for entity_id, entity in self.entities.items():
            xml_lines.append(f'    <node id="{entity_id}">')
            xml_lines.append(f'      <data key="type">{entity.type}</data>')
            xml_lines.append(f'      <data key="name">{entity.name}</data>')
            xml_lines.append('    </node>')
        
        # Aristas
        for rel in self.relationships:
            xml_lines.append(
                f'    <edge source="{rel.source_id}" target="{rel.target_id}">'
            )
            xml_lines.append(f'      <data key="type">{rel.type}</data>')
            xml_lines.append('    </edge>')
        
        xml_lines.append('  </graph>')
        xml_lines.append('</graphml>')
        
        return '\n'.join(xml_lines)
    
    def _to_cypher(self) -> str:
        """Exporta a formato Cypher (Neo4j)."""
        cypher_lines = []
        
        # Crear nodos
        for entity_id, entity in self.entities.items():
            props = entity.dict(exclude={'id', 'type'})
            # Escapar comillas en strings
            for key, value in props.items():
                if isinstance(value, str):
                    props[key] = value.replace("'", "\\'").replace('"', '\\"')
            
            props_str = ', '.join(f'{k}: "{v}"' if isinstance(v, str) else f'{k}: {v}'
                                 for k, v in props.items() if v is not None)
            
            cypher_lines.append(
                f'CREATE ({entity_id}:{entity.type.value} {{id: "{entity_id}", {props_str}}})'
            )
        
        # Crear relaciones
        for rel in self.relationships:
            cypher_lines.append(
                f'MATCH (a {{id: "{rel.source_id}"}}), (b {{id: "{rel.target_id}"}}) '
                f'CREATE (a)-[:{rel.type.value} {{weight: {rel.weight}}}]->(b)'
            )
        
        return ';\n'.join(cypher_lines) + ';'
6.2 Esquema de Base de Datos PostgreSQL

sql
-- ============================================
-- ESQUEMA DE BASE DE DATOS - PROJECT BRAIN
-- ============================================

-- Tabla principal de proyectos
CREATE TABLE projects (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    path TEXT NOT NULL,
    description TEXT,
    language VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    last_analyzed TIMESTAMP WITH TIME ZONE,
    analysis_status VARCHAR(50) DEFAULT 'pending',
    metadata JSONB DEFAULT '{}',
    UNIQUE(path)
);

-- √çndices para proyectos
CREATE INDEX idx_projects_name ON projects(name);
CREATE INDEX idx_projects_language ON projects(language);
CREATE INDEX idx_projects_status ON projects(analysis_status);
CREATE INDEX idx_projects_metadata ON projects USING gin(metadata);

-- Tabla de archivos analizados
CREATE TABLE files (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    project_id UUID NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    path TEXT NOT NULL,
    name VARCHAR(255) NOT NULL,
    extension VARCHAR(20),
    language VARCHAR(50),
    size_bytes INTEGER NOT NULL,
    line_count INTEGER NOT NULL,
    content_hash CHAR(64) NOT NULL,
    encoding VARCHAR(50) DEFAULT 'utf-8',
    
    -- M√©tricas
    complexity_level VARCHAR(20),
    maintainability_index DECIMAL(5,2),
    test_coverage DECIMAL(5,2),
    issue_count INTEGER DEFAULT 0,
    
    -- An√°lisis
    parsed_ast JSONB,
    analysis_summary JSONB DEFAULT '{}',
    
    -- Timestamps
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    last_parsed TIMESTAMP WITH TIME ZONE,
    
    -- Constraints
    UNIQUE(project_id, path),
    CHECK(size_bytes >= 0),
    CHECK(line_count >= 0),
    CHECK(issue_count >= 0)
);

-- √çndices para archivos
CREATE INDEX idx_files_project ON files(project_id);
CREATE INDEX idx_files_path ON files(path);
CREATE INDEX idx_files_extension ON files(extension);
CREATE INDEX idx_files_language ON files(language);
CREATE INDEX idx_files_hash ON files(content_hash);
CREATE INDEX idx_files_complexity ON files(complexity_level);
CREATE INDEX idx_files_ast ON files USING gin(parsed_ast);
CREATE INDEX idx_files_summary ON files USING gin(analysis_summary);

-- Tabla de funciones/m√©todos
CREATE TABLE functions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    file_id UUID NOT NULL REFERENCES files(id) ON DELETE CASCADE,
    project_id UUID NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    name VARCHAR(255) NOT NULL,
    signature TEXT NOT NULL,
    return_type VARCHAR(100),
    
    -- Localizaci√≥n
    start_line INTEGER NOT NULL,
    end_line INTEGER NOT NULL,
    start_column INTEGER,
    end_column INTEGER,
    
    -- Propiedades
    is_async BOOLEAN DEFAULT FALSE,
    is_generator BOOLEAN DEFAULT FALSE,
    is_coroutine BOOLEAN DEFAULT FALSE,
    decorators TEXT[] DEFAULT '{}',
    docstring TEXT,
    
    -- M√©tricas
    cyclomatic_complexity INTEGER DEFAULT 1,
    cognitive_complexity INTEGER DEFAULT 0,
    lines_of_code INTEGER DEFAULT 0,
    parameter_count INTEGER DEFAULT 0,
    
    -- An√°lisis
    calls JSONB DEFAULT '[]',
    variables JSONB DEFAULT '[]',
    issues JSONB DEFAULT '[]',
    
    -- Metadata
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    
    -- Constraints
    CHECK(start_line >= 1),
    CHECK(end_line >= start_line),
    CHECK(cyclomatic_complexity >= 1),
    CHECK(lines_of_code >= 0),
    CHECK(parameter_count >= 0)
);

-- √çndices para funciones
CREATE INDEX idx_functions_file ON functions(file_id);
CREATE INDEX idx_functions_project ON functions(project_id);
CREATE INDEX idx_functions_name ON functions(name);
CREATE INDEX idx_functions_complexity ON functions(cyclomatic_complexity);
CREATE INDEX idx_functions_location ON functions(start_line, end_line);
CREATE INDEX idx_functions_calls ON functions USING gin(calls);
CREATE UNIQUE INDEX idx_functions_unique 
    ON functions(file_id, signature, start_line);

-- Tabla de clases
CREATE TABLE classes (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    file_id UUID NOT NULL REFERENCES files(id) ON DELETE CASCADE,
    project_id UUID NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    name VARCHAR(255) NOT NULL,
    full_name TEXT,
    
    -- Herencia
    bases TEXT[] DEFAULT '{}',
    
    -- Propiedades
    decorators TEXT[] DEFAULT '{}',
    docstring TEXT,
    is_abstract BOOLEAN DEFAULT FALSE,
    is_final BOOLEAN DEFAULT FALSE,
    is_dataclass BOOLEAN DEFAULT FALSE,
    access_modifier VARCHAR(20) DEFAULT 'public',
    
    -- M√©tricas
    method_count INTEGER DEFAULT 0,
    attribute_count INTEGER DEFAULT 0,
    inheritance_depth INTEGER DEFAULT 0,
    cohesion_score DECIMAL(5,4),
    
    -- Miembros (referencias)
    methods JSONB DEFAULT '[]',
    attributes JSONB DEFAULT '[]',
    properties JSONB DEFAULT '[]',
    
    -- An√°lisis
    issues JSONB DEFAULT '[]',
    design_patterns JSONB DEFAULT '[]',
    
    -- Metadata
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- √çndices para clases
CREATE INDEX idx_classes_file ON classes(file_id);
CREATE INDEX idx_classes_project ON classes(project_id);
CREATE INDEX idx_classes_name ON classes(name);
CREATE INDEX idx_classes_method_count ON classes(method_count);
CREATE INDEX idx_classes_patterns ON classes USING gin(design_patterns);

-- Tabla de imports/dependencias
CREATE TABLE imports (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    file_id UUID NOT NULL REFERENCES files(id) ON DELETE CASCADE,
    project_id UUID NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    module TEXT NOT NULL,
    imports TEXT[] DEFAULT '{}',
    alias VARCHAR(255),
    is_relative BOOLEAN DEFAULT FALSE,
    level INTEGER DEFAULT 0,
    
    -- Uso
    is_used BOOLEAN DEFAULT FALSE,
    usage_count INTEGER DEFAULT 0,
    line_number INTEGER NOT NULL,
    
    -- Metadata
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    
    CHECK(level >= 0),
    CHECK(usage_count >= 0),
    CHECK(line_number >= 1)
);

-- √çndices para imports
CREATE INDEX idx_imports_file ON imports(file_id);
CREATE INDEX idx_imports_project ON imports(project_id);
CREATE INDEX idx_imports_module ON imports(module);
CREATE INDEX idx_imports_used ON imports(is_used);

-- Tabla de issues encontrados
CREATE TABLE issues (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    project_id UUID NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    entity_id UUID,  -- Puede referenciar file, function, class, etc.
    entity_type VARCHAR(50) NOT NULL,
    
    -- Detalles del issue
    issue_type VARCHAR(100) NOT NULL,
    severity VARCHAR(20) NOT NULL,
    message TEXT NOT NULL,
    
    -- Ubicaci√≥n
    file_path TEXT NOT NULL,
    start_line INTEGER NOT NULL,
    end_line INTEGER,
    start_column INTEGER,
    end_column INTEGER,
    code_snippet TEXT,
    
    -- Metadata del issue
    rule_id VARCHAR(100),
    rule_category VARCHAR(100),
    effort_minutes INTEGER,
    tags TEXT[] DEFAULT '{}',
    
    -- An√°lisis
    confidence DECIMAL(3,2) DEFAULT 0.8,
    is_false_positive BOOLEAN DEFAULT FALSE,
    is_fixed BOOLEAN DEFAULT FALSE,
    
    -- Sugerencias
    suggestion TEXT,
    example_fix TEXT,
    references TEXT[] DEFAULT '{}',
    
    -- Timestamps
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    fixed_at TIMESTAMP WITH TIME ZONE,
    
    -- Constraints
    CHECK(confidence >= 0 AND confidence <= 1),
    CHECK(severity IN ('info', 'low', 'medium', 'high', 'critical')),
    CHECK(start_line >= 1),
    CHECK(end_line IS NULL OR end_line >= start_line),
    CHECK(effort_minutes IS NULL OR effort_minutes >= 0)
);

-- √çndices para issues
CREATE INDEX idx_issues_project ON issues(project_id);
CREATE INDEX idx_issues_entity ON issues(entity_id, entity_type);
CREATE INDEX idx_issues_severity ON issues(severity);
CREATE INDEX idx_issues_type ON issues(issue_type);
CREATE INDEX idx_issues_fixed ON issues(is_fixed);
CREATE INDEX idx_issues_false_positive ON issues(is_false_positive);
CREATE INDEX idx_issues_confidence ON issues(confidence);
CREATE INDEX idx_issues_tags ON issues USING gin(tags);
CREATE INDEX idx_issues_location ON issues(file_path, start_line);

-- Tabla de relaciones entre entidades
CREATE TABLE relationships (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    project_id UUID NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    
    -- Entidades relacionadas
    source_id UUID NOT NULL,
    source_type VARCHAR(50) NOT NULL,
    target_id UUID NOT NULL,
    target_type VARCHAR(50) NOT NULL,
    
    -- Tipo de relaci√≥n
    relationship_type VARCHAR(50) NOT NULL,
    
    -- Propiedades
    weight DECIMAL(3,2) DEFAULT 1.0,
    confidence DECIMAL(3,2) DEFAULT 1.0,
    
    -- Metadata
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    
    -- Constraints
    CHECK(weight >= 0 AND weight <= 1),
    CHECK(confidence >= 0 AND confidence <= 1)
);

-- √çndices para relaciones
CREATE INDEX idx_relationships_project ON relationships(project_id);
CREATE INDEX idx_relationships_source ON relationships(source_id, source_type);
CREATE INDEX idx_relationships_target ON relationships(target_id, target_type);
CREATE INDEX idx_relationships_type ON relationships(relationship_type);
CREATE INDEX idx_relationships_weight ON relationships(weight);
CREATE INDEX idx_relationships_metadata ON relationships USING gin(metadata);

-- Tabla de embeddings vectoriales
CREATE TABLE embeddings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    project_id UUID NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    entity_id UUID NOT NULL,
    entity_type VARCHAR(50) NOT NULL,
    
    -- Embedding
    embedding_vector VECTOR(384),  -- Dimensi√≥n para all-MiniLM-L6-v2
    embedding_model VARCHAR(100) NOT NULL,
    embedding_type VARCHAR(50) NOT NULL,  -- code, text, combined
    
    -- Metadata
    content_hash CHAR(64) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    
    -- Constraints
    UNIQUE(project_id, entity_id, entity_type, embedding_type)
);

-- √çndices para embeddings (usando pgvector si est√° disponible)
CREATE INDEX idx_embeddings_project ON embeddings(project_id);
CREATE INDEX idx_embeddings_entity ON embeddings(entity_id, entity_type);
CREATE INDEX idx_embeddings_type ON embeddings(embedding_type);
-- √çndice para b√∫squeda por similitud (requiere pgvector)
-- CREATE INDEX idx_embeddings_vector ON embeddings USING ivfflat (embedding_vector);

-- Tabla de interacciones con usuarios
CREATE TABLE interactions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    project_id UUID REFERENCES projects(id) ON DELETE SET NULL,
    session_id UUID NOT NULL,
    user_id VARCHAR(255),  -- Opcional, para multi-usuario
    
    -- Pregunta/respuesta
    question TEXT NOT NULL,
    answer TEXT,
    answer_structured JSONB,
    
    -- Contexto
    context JSONB DEFAULT '{}',
    relevant_entities JSONB DEFAULT '[]',
    
    -- M√©tricas
    confidence DECIMAL(3,2),
    processing_time_ms INTEGER,
    feedback_score INTEGER,  -- 1-5
    
    -- Metadata
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    
    -- Constraints
    CHECK(confidence IS NULL OR (confidence >= 0 AND confidence <= 1)),
    CHECK(feedback_score IS NULL OR (feedback_score >= 1 AND feedback_score <= 5)),
    CHECK(processing_time_ms IS NULL OR processing_time_ms >= 0)
);

-- √çndices para interacciones
CREATE INDEX idx_interactions_project ON interactions(project_id);
CREATE INDEX idx_interactions_session ON interactions(session_id);
CREATE INDEX idx_interactions_user ON interactions(user_id);
CREATE INDEX idx_interactions_created ON interactions(created_at);
CREATE INDEX idx_interactions_feedback ON interactions(feedback_score);

-- Tabla de aprendizaje del sistema
CREATE TABLE learning_events (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    project_id UUID REFERENCES projects(id) ON DELETE SET NULL,
    
    -- Evento
    event_type VARCHAR(100) NOT NULL,
    event_data JSONB NOT NULL,
    
    -- Resultado del aprendizaje
    knowledge_gained JSONB,
    confidence_delta DECIMAL(4,3),  -- Cambio en confianza
    
    -- Metadata
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    
    CHECK(confidence_delta IS NULL OR (confidence_delta >= -1 AND confidence_delta <= 1))
);

-- √çndices para aprendizaje
CREATE INDEX idx_learning_events_project ON learning_events(project_id);
CREATE INDEX idx_learning_events_type ON learning_events(event_type);
CREATE INDEX idx_learning_events_created ON learning_events(created_at);

-- Tabla de m√©tricas del sistema
CREATE TABLE system_metrics (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    metric_name VARCHAR(100) NOT NULL,
    metric_value DECIMAL(12,4) NOT NULL,
    
    -- Dimensiones
    project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
    component VARCHAR(100),
    tags JSONB DEFAULT '{}',
    
    -- Timestamp
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- √çndices para m√©tricas
CREATE INDEX idx_system_metrics_name ON system_metrics(metric_name);
CREATE INDEX idx_system_metrics_project ON system_metrics(project_id);
CREATE INDEX idx_system_metrics_component ON system_metrics(component);
CREATE INDEX idx_system_metrics_timestamp ON system_metrics(timestamp);
CREATE INDEX idx_system_metrics_tags ON system_metrics USING gin(tags);

-- Vista para reportes de proyecto
CREATE VIEW project_reports AS
SELECT 
    p.id as project_id,
    p.name as project_name,
    p.language as main_language,
    p.created_at as project_created,
    p.last_analyzed as last_analysis,
    
    -- Estad√≠sticas de archivos
    COUNT(DISTINCT f.id) as file_count,
    SUM(f.size_bytes) as total_size_bytes,
    SUM(f.line_count) as total_lines,
    AVG(f.maintainability_index) as avg_maintainability,
    
    -- Estad√≠sticas de funciones
    COUNT(DISTINCT fn.id) as function_count,
    AVG(fn.cyclomatic_complexity) as avg_complexity,
    MAX(fn.cyclomatic_complexity) as max_complexity,
    
    -- Estad√≠sticas de issues
    COUNT(DISTINCT i.id) as issue_count,
    COUNT(DISTINCT CASE WHEN i.severity = 'critical' THEN i.id END) as critical_issues,
    COUNT(DISTINCT CASE WHEN i.severity = 'high' THEN i.id END) as high_issues,
    
    -- Estad√≠sticas de relaciones
    COUNT(DISTINCT r.id) as relationship_count
    
FROM projects p
LEFT JOIN files f ON p.id = f.project_id
LEFT JOIN functions fn ON p.id = fn.project_id
LEFT JOIN issues i ON p.id = i.project_id AND i.is_fixed = FALSE
LEFT JOIN relationships r ON p.id = r.project_id
GROUP BY p.id, p.name, p.language, p.created_at, p.last_analyzed;

-- Funci√≥n para actualizar updated_at autom√°ticamente
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ language 'plpgsql';

-- Triggers para updated_at
CREATE TRIGGER update_projects_updated_at 
    BEFORE UPDATE ON projects 
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_files_updated_at 
    BEFORE UPDATE ON files 
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_functions_updated_at 
    BEFORE UPDATE ON functions 
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_classes_updated_at 
    BEFORE UPDATE ON classes 
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_issues_updated_at 
    BEFORE UPDATE ON issues 
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- Funci√≥n para limpiar datos antiguos
CREATE OR REPLACE FUNCTION cleanup_old_data(retention_days INTEGER DEFAULT 90)
RETURNS INTEGER AS $$
DECLARE
    deleted_count INTEGER;
BEGIN
    -- Eliminar interacciones antiguas
    DELETE FROM interactions 
    WHERE created_at < CURRENT_TIMESTAMP - INTERVAL '1 day' * retention_days;
    
    GET DIAGNOSTICS deleted_count = ROW_COUNT;
    
    -- Eliminar eventos de aprendizaje antiguos
    DELETE FROM learning_events 
    WHERE created_at < CURRENT_TIMESTAMP - INTERVAL '1 day' * retention_days;
    
    GET DIAGNOSTICS deleted_count = deleted_count + ROW_COUNT;
    
    -- Eliminar m√©tricas antiguas (mantener solo res√∫menes diarios)
    DELETE FROM system_metrics 
    WHERE timestamp < CURRENT_TIMESTAMP - INTERVAL '1 day' * retention_days
    AND metric_name NOT LIKE 'daily_%';
    
    GET DIAGNOSTICS deleted_count = deleted_count + ROW_COUNT;
    
    RETURN deleted_count;
END;
$$ LANGUAGE plpgsql;

-- √çndice para limpieza eficiente
CREATE INDEX idx_cleanup_interactions ON interactions(created_at);
CREATE INDEX idx_cleanup_learning ON learning_events(created_at);
CREATE INDEX idx_cleanup_metrics ON system_metrics(timestamp);
7. ESPECIFICACIONES DE API COMPLETAS

7.1 API REST - Especificaci√≥n OpenAPI 3.0

yaml
openapi: 3.0.3
info:
  title: Project Brain API
  description: |
    API completa para el sistema Project Brain (AnalyzerBrain).
    Sistema de inteligencia artificial para an√°lisis y comprensi√≥n de c√≥digo.
  version: 1.0.0
  contact:
    name: Project Brain Team
    email: support@projectbrain.dev
  license:
    name: MIT
    url: https://opensource.org/licenses/MIT

servers:
  - url: https://api.projectbrain.dev/v1
    description: Production server
  - url: http://localhost:8000/v1
    description: Development server

tags:
  - name: Projects
    description: Operaciones con proyectos
  - name: Analysis
    description: An√°lisis de c√≥digo
  - name: Query
    description: Consultas sobre proyectos
  - name: Knowledge
    description: Gesti√≥n del conocimiento
  - name: Agents
    description: Agentes de IA
  - name: System
    description: Estado del sistema

paths:
  # ========== PROJECTS ==========
  /projects:
    get:
      tags: [Projects]
      summary: Listar proyectos
      description: Obtiene lista de proyectos analizados
      parameters:
        - name: page
          in: query
          schema:
            type: integer
            minimum: 1
            default: 1
        - name: page_size
          in: query
          schema:
            type: integer
            minimum: 1
            maximum: 100
            default: 20
        - name: language
          in: query
          schema:
            type: string
            enum: [python, javascript, typescript, java, cpp, go, rust]
        - name: status
          in: query
          schema:
            type: string
            enum: [pending, analyzing, completed, failed]
      responses:
        200:
          description: Lista de proyectos
          content:
            application/json:
              schema:
                type: object
                properties:
                  projects:
                    type: array
                    items:
                      $ref: '#/components/schemas/Project'
                  pagination:
                    $ref: '#/components/schemas/Pagination'
        400:
          $ref: '#/components/responses/BadRequest'
        500:
          $ref: '#/components/responses/InternalError'
    
    post:
      tags: [Projects]
      summary: Crear proyecto
      description: Crea un nuevo proyecto para an√°lisis
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required: [name, path]
              properties:
                name:
                  type: string
                  description: Nombre del proyecto
                  example: "My Python Project"
                path:
                  type: string
                  description: Ruta al proyecto
                  example: "/home/user/projects/myapp"
                description:
                  type: string
                  description: Descripci√≥n opcional
                language:
                  type: string
                  description: Lenguaje principal
                  enum: [python, javascript, typescript, java, cpp, go, rust]
                options:
                  $ref: '#/components/schemas/AnalysisOptions'
      responses:
        201:
          description: Proyecto creado exitosamente
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Project'
        400:
          $ref: '#/components/responses/BadRequest'
        409:
          description: El proyecto ya existe
        500:
          $ref: '#/components/responses/InternalError'

  /projects/{project_id}:
    get:
      tags: [Projects]
      summary: Obtener proyecto
      description: Obtiene detalles de un proyecto espec√≠fico
      parameters:
        - name: project_id
          in: path
          required: true
          schema:
            type: string
            format: uuid
      responses:
        200:
          description: Detalles del proyecto
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Project'
        404:
          $ref: '#/components/responses/NotFound'
        500:
          $ref: '#/components/responses/InternalError'
    
    delete:
      tags: [Projects]
      summary: Eliminar proyecto
      description: Elimina un proyecto y todo su conocimiento asociado
      parameters:
        - name: project_id
          in: path
          required: true
          schema:
            type: string
            format: uuid
        - name: force
          in: query
          schema:
            type: boolean
            default: false
      responses:
        204:
          description: Proyecto eliminado exitosamente
        404:
          $ref: '#/components/responses/NotFound'
        500:
          $ref: '#/components/responses/InternalError'

  /projects/{project_id}/analyze:
    post:
      tags: [Projects, Analysis]
      summary: Analizar proyecto
      description: Inicia o re-ejecuta an√°lisis de un proyecto
      parameters:
        - name: project_id
          in: path
          required: true
          schema:
            type: string
            format: uuid
      requestBody:
        required: false
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/AnalysisOptions'
      responses:
        202:
          description: An√°lisis iniciado
          content:
            application/json:
              schema:
                type: object
                properties:
                  analysis_id:
                    type: string
                    format: uuid
                  status_url:
                    type: string
                    format: uri
                    example: "/v1/analysis/{analysis_id}/status"
                  estimated_time_seconds:
                    type: integer
        404:
          $ref: '#/components/responses/NotFound'
        409:
          description: An√°lisis ya en progreso
        500:
          $ref: '#/components/responses/InternalError'

  # ========== ANALYSIS ==========
  /analysis/{analysis_id}/status:
    get:
      tags: [Analysis]
      summary: Estado de an√°lisis
      description: Obtiene estado de un an√°lisis en progreso
      parameters:
        - name: analysis_id
          in: path
          required: true
          schema:
            type: string
            format: uuid
      responses:
        200:
          description: Estado del an√°lisis
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AnalysisStatus'
        404:
          $ref: '#/components/responses/NotFound'
        500:
          $ref: '#/components/responses/InternalError'

  /analysis/{analysis_id}/cancel:
    post:
      tags: [Analysis]
      summary: Cancelar an√°lisis
      description: Cancela un an√°lisis en progreso
      parameters:
        - name: analysis_id
          in: path
          required: true
          schema:
            type: string
            format: uuid
      responses:
        200:
          description: An√°lisis cancelado
        404:
          $ref: '#/components/responses/NotFound'
        500:
          $ref: '#/components/responses/InternalError'

  # ========== QUERY ==========
  /query:
    post:
      tags: [Query]
      summary: Consultar proyecto
      description: Realiza una consulta sobre un proyecto
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required: [question]
              properties:
                question:
                  type: string
                  description: Pregunta en lenguaje natural
                  example: "¬øQu√© hace la funci√≥n process_data?"
                project_id:
                  type: string
                  format: uuid
                  description: ID del proyecto (opcional si hay sesi√≥n)
                context:
                  $ref: '#/components/schemas/QueryContext'
                options:
                  $ref: '#/components/schemas/QueryOptions'
      responses:
        200:
          description: Respuesta generada
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/QueryResponse'
        400:
          $ref: '#/components/responses/BadRequest'
        404:
          $ref: '#/components/responses/NotFound'
        500:
          $ref: '#/components/responses/InternalError'

  /query/conversation:
    post:
      tags: [Query]
      summary: Consulta conversacional
      description: Consulta con historial de conversaci√≥n
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required: [question, session_id]
              properties:
                question:
                  type: string
                session_id:
                  type: string
                  format: uuid
                project_id:
                  type: string
                  format: uuid
                context:
                  $ref: '#/components/schemas/QueryContext'
      responses:
        200:
          description: Respuesta con contexto de conversaci√≥n
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ConversationResponse'
        400:
          $ref: '#/components/responses/BadRequest'
        500:
          $ref: '#/components/responses/InternalError'

  # ========== KNOWLEDGE ==========
  /projects/{project_id}/knowledge/graph:
    get:
      tags: [Knowledge]
      summary: Exportar grafo de conocimiento
      description: Exporta el grafo de conocimiento del proyecto
      parameters:
        - name: project_id
          in: path
          required: true
          schema:
            type: string
            format: uuid
        - name: format
          in: query
          schema:
            type: string
            enum: [json, graphml, cypher, dot]
            default: json
        - name: depth
          in: query
          schema:
            type: integer
            minimum: 1
            maximum: 5
            default: 2
        - name: include
          in: query
          schema:
            type: string
            enum: [nodes, edges, properties, all]
            default: all
      responses:
        200:
          description: Grafo exportado
          content:
            application/json:
              schema:
                type: object
                properties:
                  format:
                    type: string
                  data:
                    oneOf:
                      - type: object
                      - type: string
        404:
          $ref: '#/components/responses/NotFound'
        500:
          $ref: '#/components/responses/InternalError'

  /projects/{project_id}/knowledge/search:
    post:
      tags: [Knowledge]
      summary: Buscar en conocimiento
      description: B√∫squeda sem√°ntica en el conocimiento del proyecto
      parameters:
        - name: project_id
          in: path
          required: true
          schema:
            type: string
            format: uuid
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required: [query]
              properties:
                query:
                  type: string
                  description: Consulta de b√∫squeda
                type:
                  type: string
                  enum: [semantic, keyword, hybrid]
                  default: hybrid
                limit:
                  type: integer
                  minimum: 1
                  maximum: 100
                  default: 10
                filters:
                  type: object
                  description: Filtros por tipo de entidad, etc.
      responses:
        200:
          description: Resultados de b√∫squeda
          content:
            application/json:
              schema:
                type: object
                properties:
                  results:
                    type: array
                    items:
                      $ref: '#/components/schemas/SearchResult'
                  total:
                    type: integer
        404:
          $ref: '#/components/responses/NotFound'
        500:
          $ref: '#/components/responses/InternalError'

  # ========== AGENTS ==========
  /agents:
    get:
      tags: [Agents]
      summary: Listar agentes
      description: Obtiene lista de agentes disponibles
      responses:
        200:
          description: Lista de agentes
          content:
            application/json:
              schema:
                type: object
                properties:
                  agents:
                    type: array
                    items:
                      $ref: '#/components/schemas/Agent'
        500:
          $ref: '#/components/responses/InternalError'

  /agents/{agent_id}/capabilities:
    get:
      tags: [Agents]
      summary: Capacidades de agente
      description: Obtiene capacidades detalladas de un agente
      parameters:
        - name: agent_id
          in: path
          required: true
          schema:
            type: string
      responses:
        200:
          description: Capacidades del agente
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AgentCapabilities'
        404:
          $ref: '#/components/responses/NotFound'
        500:
          $ref: '#/components/responses/InternalError'

  # ========== SYSTEM ==========
  /system/health:
    get:
      tags: [System]
      summary: Salud del sistema
      description: Verifica la salud del sistema y sus componentes
      responses:
        200:
          description: Estado de salud
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/HealthStatus'
        503:
          description: Sistema no saludable
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/HealthStatus'

  /system/metrics:
    get:
      tags: [System]
      summary: M√©tricas del sistema
      description: Obtiene m√©tricas de performance del sistema
      parameters:
        - name: timeframe
          in: query
          schema:
            type: string
            enum: [hour, day, week, month]
            default: hour
        - name: component
          in: query
          schema:
            type: string
      responses:
        200:
          description: M√©tricas del sistema
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SystemMetrics'
        500:
          $ref: '#/components/responses/InternalError'

  /system/status:
    get:
      tags: [System]
      summary: Estado del sistema
      description: Estado detallado del sistema
      responses:
        200:
          description: Estado del sistema
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SystemStatus'
        500:
          $ref: '#/components/responses/InternalError'

components:
  schemas:
    # ========== CORE SCHEMAS ==========
    Project:
      type: object
      required: [id, name, path, created_at]
      properties:
        id:
          type: string
          format: uuid
        name:
          type: string
        path:
          type: string
        description:
          type: string
        language:
          type: string
          enum: [python, javascript, typescript, java, cpp, go, rust]
        created_at:
          type: string
          format: date-time
        updated_at:
          type: string
          format: date-time
        last_analyzed:
          type: string
          format: date-time
        analysis_status:
          type: string
          enum: [pending, analyzing, completed, failed]
        metadata:
          type: object
        stats:
          $ref: '#/components/schemas/ProjectStats'

    ProjectStats:
      type: object
      properties:
        file_count:
          type: integer
        total_lines:
          type: integer
        function_count:
          type: integer
        class_count:
          type: integer
        issue_count:
          type: integer
        critical_issues:
          type: integer
        avg_complexity:
          type: number
          format: float

    AnalysisOptions:
      type: object
      properties:
        mode:
          type: string
          enum: [quick, standard, comprehensive, deep]
          default: comprehensive
        include_tests:
          type: boolean
          default: true
        include_docs:
          type: boolean
          default: true
        max_file_size_mb:
          type: integer
          default: 10
        timeout_minutes:
          type: integer
          default: 30
        languages:
          type: array
          items:
            type: string
          default: [python, javascript, typescript, java, cpp, go, rust]

    AnalysisStatus:
      type: object
      required: [id, status]
      properties:
        id:
          type: string
          format: uuid
        status:
          type: string
          enum: [pending, running, completed, failed, cancelled]
        progress:
          type: number
          format: float
          minimum: 0
          maximum: 100
        current_step:
          type: string
        estimated_remaining_seconds:
          type: integer
        started_at:
          type: string
          format: date-time
        completed_at:
          type: string
          format: date-time
        results:
          type: object
          nullable: true
        errors:
          type: array
          items:
            type: string

    # ========== QUERY SCHEMAS ==========
    QueryContext:
      type: object
      properties:
        current_file:
          type: string
        current_function:
          type: string
        user_role:
          type: string
          enum: [developer, architect, manager, qa]
        technical_level:
          type: string
          enum: [beginner, intermediate, advanced, expert]

    QueryOptions:
      type: object
      properties:
        detail_level:
          type: string
          enum: [brief, normal, detailed]
          default: normal
        include_code:
          type: boolean
          default: true
        include_explanations:
          type: boolean
          default: true
        include_sources:
          type: boolean
          default: true
        max_tokens:
          type: integer
          default: 2000

    QueryResponse:
      type: object
      required: [answer, confidence]
      properties:
        answer:
          $ref: '#/components/schemas/Answer'
        confidence:
          type: number
          format: float
          minimum: 0
          maximum: 1
        sources:
          type: array
          items:
            $ref: '#/components/schemas/Source'
        reasoning_chain:
          type: array
          items:
            type: string
        suggested_followups:
          type: array
          items:
            type: string
        processing_time_ms:
          type: number
          format: float
        metadata:
          type: object

    Answer:
      type: object
      properties:
        text:
          type: string
        structured:
          type: object
        code_examples:
          type: array
          items:
            $ref: '#/components/schemas/CodeExample'
        related_concepts:
          type: array
          items:
            type: string

    Source:
      type: object
      properties:
        type:
          type: string
          enum: [code, documentation, knowledge, external]
        file_path:
          type: string
        line_range:
          type: array
          items:
            type: integer
          minItems: 2
          maxItems: 2
        confidence:
          type: number
          format: float
        excerpt:
          type: string

    CodeExample:
      type: object
      properties:
        code:
          type: string
        language:
          type: string
        description:
          type: string
        line_numbers:
          type: boolean
          default: true

    ConversationResponse:
      allOf:
        - $ref: '#/components/schemas/QueryResponse'
        - type: object
          properties:
            session_id:
              type: string
              format: uuid
            conversation_history:
              type: array
              items:
                $ref: '#/components/schemas/ConversationTurn'
            context_updated:
              type: boolean

    ConversationTurn:
      type: object
      properties:
        role:
          type: string
          enum: [user, assistant, system]
        content:
          type: string
        timestamp:
          type: string
          format: date-time
        confidence:
          type: number
          format: float

    # ========== KNOWLEDGE SCHEMAS ==========
    SearchResult:
      type: object
      properties:
        id:
          type: string
        type:
          type: string
          enum: [file, function, class, variable, concept]
        name:
          type: string
        score:
          type: number
          format: float
        content:
          type: string
        metadata:
          type: object

    # ========== AGENT SCHEMAS ==========
    Agent:
      type: object
      properties:
        id:
          type: string
        name:
          type: string
        description:
          type: string
        version:
          type: string
        status:
          type: string
          enum: [initializing, ready, processing, error, maintenance]
        capabilities:
          type: array
          items:
            type: string
        metrics:
          type: object

    AgentCapabilities:
      type: object
      properties:
        agent_id:
          type: string
        capabilities:
          type: array
          items:
            $ref: '#/components/schemas/CapabilityDetail'

    CapabilityDetail:
      type: object
      properties:
        name:
          type: string
        description:
          type: string
        confidence_threshold:
          type: number
          format: float
        supported_languages:
          type: array
          items:
            type: string
        examples:
          type: array
          items:
            type: object

    # ========== SYSTEM SCHEMAS ==========
    HealthStatus:
      type: object
      required: [status, timestamp]
      properties:
        status:
          type: string
          enum: [healthy, degraded, unhealthy]
        timestamp:
          type: string
          format: date-time
        components:
          type: object
          additionalProperties:
            $ref: '#/components/schemas/ComponentHealth'
        uptime_seconds:
          type: number
          format: float
        version:
          type: string

    ComponentHealth:
      type: object
      properties:
        status:
          type: string
          enum: [healthy, degraded, unhealthy]
        message:
          type: string
        latency_ms:
          type: number
          format: float
        last_check:
          type: string
          format: date-time

    SystemMetrics:
      type: object
      properties:
        timestamp:
          type: string
          format: date-time
        timeframe:
          type: string
        metrics:
          type: object
          properties:
            performance:
              $ref: '#/components/schemas/PerformanceMetrics'
            resources:
              $ref: '#/components/schemas/ResourceMetrics'
            business:
              $ref: '#/components/schemas/BusinessMetrics'

    PerformanceMetrics:
      type: object
      properties:
        api_response_time_p95_ms:
          type: number
          format: float
        question_processing_time_p95_ms:
          type: number
          format: float
        analysis_throughput_files_per_second:
          type: number
          format: float
        cache_hit_rate:
          type: number
          format: float
        error_rate:
          type: number
          format: float

    ResourceMetrics:
      type: object
      properties:
        cpu_usage_percent:
          type: number
          format: float
        memory_usage_percent:
          type: number
          format: float
        disk_usage_percent:
          type: number
          format: float
        active_connections:
          type: integer
        queue_length:
          type: integer

    BusinessMetrics:
      type: object
      properties:
        active_projects:
          type: integer
        questions_answered:
          type: integer
        analysis_completed:
          type: integer
        user_satisfaction_score:
          type: number
          format: float
        knowledge_growth_rate:
          type: number
          format: float

    SystemStatus:
      type: object
      properties:
        status:
          type: string
          enum: [starting, running, stopping, maintenance, error]
        mode:
          type: string
          enum: [development, staging, production]
        version:
          type: string
        uptime_seconds:
          type: number
          format: float
        components:
          type: object
        active_operations:
          type: integer
        resource_usage:
          type: object

    # ========== COMMON ==========
    Pagination:
      type: object
      properties:
        page:
          type: integer
        page_size:
          type: integer
        total_pages:
          type: integer
        total_items:
          type: integer
        has_next:
          type: boolean
        has_previous:
          type: boolean

    Error:
      type: object
      required: [code, message]
      properties:
        code:
          type: string
        message:
          type: string
        details:
          type: object
        request_id:
          type: string

  responses:
    BadRequest:
      description: Solicitud inv√°lida
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'
          example:
            code: "VALIDATION_ERROR"
            message: "Invalid request parameters"
            details:
              field: "project_path"
              error: "Path does not exist"

    NotFound:
      description: Recurso no encontrado
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'
          example:
            code: "NOT_FOUND"
            message: "Project not found"
            details:
              project_id: "123e4567-e89b-12d3-a456-426614174000"

    InternalError:
      description: Error interno del servidor
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'
          example:
            code: "INTERNAL_ERROR"
            message: "An unexpected error occurred"
            request_id: "req_123456789"

  securitySchemes:
    ApiKeyAuth:
      type: apiKey
      in: header
      name: X-API-Key
    BearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT

security:
  - ApiKeyAuth: []
  - BearerAuth: []
7.2 WebSocket API - Protocolo de Comunicaci√≥n

python
"""
WebSocket API - Comunicaci√≥n en tiempo real.
Protocolo basado en JSON con mensajes estructurados.
"""

from typing import Dict, List, Optional, Any
from enum import Enum
from pydantic import BaseModel, Field
import json
import asyncio
from datetime import datetime

class WebSocketMessageType(str, Enum):
    """Tipos de mensajes WebSocket."""
    CONNECT = "connect"
    CONNECTED = "connected"
    DISCONNECT = "disconnect"
    ERROR = "error"
    SUBSCRIBE = "subscribe"
    UNSUBSCRIBE = "unsubscribe"
    SUBSCRIBED = "subscribed"
    QUERY = "query"
    QUERY_RESPONSE = "query_response"
    ANALYSIS_PROGRESS = "analysis_progress"
    CHANGE_DETECTED = "change_detected"
    LEARNING_UPDATE = "learning_update"
    PING = "ping"
    PONG = "pong"

class WebSocketMessage(BaseModel):
    """Mensaje base WebSocket."""
    type: WebSocketMessageType
    data: Optional[Dict[str, Any]] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)
    timestamp: datetime = Field(default_factory=datetime.now)
    
    class Config:
        arbitrary_types_allowed = True
    
    def to_json(self) -> str:
        """Convierte a JSON."""
        return self.json()
    
    @classmethod
    def from_json(cls, json_str: str) -> 'WebSocketMessage':
        """Crea desde JSON."""
        data = json.loads(json_str)
        return cls(**data)

class ConnectionRequest(BaseModel):
    """Solicitud de conexi√≥n WebSocket."""
    session_id: Optional[str] = None
    project_id: Optional[str] = None
    capabilities: List[str] = Field(default_factory=list)
    version: str = "1.0.0"

class QueryRequest(BaseModel):
    """Solicitud de consulta via WebSocket."""
    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    question: str
    context: Optional[Dict[str, Any]] = None
    stream: bool = False  # Si True, env√≠a respuestas parciales

class QueryResponseChunk(BaseModel):
    """Fragmento de respuesta de consulta."""
    request_id: str
    chunk_type: str  # 'thinking', 'partial', 'complete', 'error'
    content: str
    is_final: bool = False
    confidence: Optional[float] = None

class AnalysisProgressUpdate(BaseModel):
    """Actualizaci√≥n de progreso de an√°lisis."""
    analysis_id: str
    step: str
    progress: float  # 0.0 - 1.0
    current_file: Optional[str] = None
    files_processed: int = 0
    total_files: int = 0
    estimated_remaining_seconds: Optional[float] = None

class ChangeDetectionNotification(BaseModel):
    """Notificaci√≥n de cambio detectado."""
    project_id: str
    changes: List[Dict[str, Any]]
    timestamp: datetime = Field(default_factory=datetime.now)

class WebSocketProtocol:
    """
    Implementaci√≥n del protocolo WebSocket para Project Brain.
    
    Caracter√≠sticas:
    1. Comunicaci√≥n full-duplex en tiempo real
    2. Suscripci√≥n a diferentes canales (topics)
    3. Streaming de respuestas para consultas largas
    4. Reconexi√≥n autom√°tica con estado
    5. Heartbeat y detecci√≥n de conexi√≥n perdida
    """
    
    def __init__(self, websocket):
        self.websocket = websocket
        self.subscriptions: Set[str] = set()
        self.session_id: Optional[str] = None
        self.project_id: Optional[str] = None
        self.connected = False
        self.last_ping = datetime.now()
        
    async def handle_message(self, message_str: str) -> None:
        """Maneja un mensaje entrante."""
        try:
            message = WebSocketMessage.from_json(message_str)
            
            if message.type == WebSocketMessageType.CONNECT:
                await self._handle_connect(message)
            elif message.type == WebSocketMessageType.SUBSCRIBE:
                await self._handle_subscribe(message)
            elif message.type == WebSocketMessageType.QUERY:
                await self._handle_query(message)
            elif message.type == WebSocketMessageType.PING:
                await self._handle_ping(message)
            elif message.type == WebSocketMessageType.DISCONNECT:
                await self._handle_disconnect(message)
            else:
                await self._send_error(f"Unknown message type: {message.type}")
                
        except Exception as e:
            await self._send_error(f"Error processing message: {str(e)}")
    
    async def _handle_connect(self, message: WebSocketMessage) -> None:
        """Maneja solicitud de conexi√≥n."""
        try:
            request = ConnectionRequest(**message.data or {})
            
            # Validar y establecer conexi√≥n
            self.session_id = request.session_id or str(uuid.uuid4())
            self.project_id = request.project_id
            self.connected = True
            
            # Enviar confirmaci√≥n
            response = WebSocketMessage(
                type=WebSocketMessageType.CONNECTED,
                data={
                    "session_id": self.session_id,
                    "capabilities_supported": [
                        "query_streaming",
                        "analysis_progress",
                        "change_notifications"
                    ],
                    "protocol_version": "1.0.0"
                }
            )
            
            await self._send_message(response)
            
            # Iniciar heartbeat
            asyncio.create_task(self._heartbeat_task())
            
        except Exception as e:
            await self._send_error(f"Connection failed: {str(e)}")
    
    async def _handle_subscribe(self, message: WebSocketMessage) -> None:
        """Maneja suscripci√≥n a canales."""
        topics = message.data.get("topics", []) if message.data else []
        
        valid_topics = {
            "analysis_progress",
            "change_detected", 
            "learning_updates",
            "system_metrics"
        }
        
        subscribed = []
        for topic in topics:
            if topic in valid_topics:
                self.subscriptions.add(topic)
                subscribed.append(topic)
        
        response = WebSocketMessage(
            type=WebSocketMessageType.SUBSCRIBED,
            data={"topics": subscribed}
        )
        
        await self._send_message(response)
    
    async def _handle_query(self, message: WebSocketMessage) -> None:
        """Maneja consulta via WebSocket."""
        try:
            request = QueryRequest(**message.data or {})
            
            # Enviar confirmaci√≥n inmediata
            await self._send_message(WebSocketMessage(
                type=WebSocketMessageType.QUERY_RESPONSE,
                data={
                    "request_id": request.request_id,
                    "chunk_type": "acknowledged",
                    "content": "Processing your question..."
                }
            ))
            
            # Procesar consulta (simulado)
            if request.stream:
                # Env√≠o en streaming
                await self._stream_query_response(request)
            else:
                # Env√≠o completo
                await self._send_complete_response(request)
                
        except Exception as e:
            await self._send_error(f"Query processing failed: {str(e)}")
    
    async def _stream_query_response(self, request: QueryRequest) -> None:
        """Env√≠a respuesta en streaming."""
        # Simular pensamiento
        thinking_steps = [
            "Analyzing your question...",
            "Searching project knowledge...",
            "Consulting specialized agents...",
            "Synthesizing answer..."
        ]
        
        for step in thinking_steps:
            await self._send_message(WebSocketMessage(
                type=WebSocketMessageType.QUERY_RESPONSE,
                data={
                    "request_id": request.request_id,
                    "chunk_type": "thinking",
                    "content": step,
                    "is_final": False
                }
            ))
            await asyncio.sleep(0.5)  # Simular procesamiento
        
        # Enviar respuesta parcial
        await self._send_message(WebSocketMessage(
            type=WebSocketMessageType.QUERY_RESPONSE,
            data={
                "request_id": request.request_id,
                "chunk_type": "partial",
                "content": "Based on my analysis, I found that...",
                "is_final": False
            }
        ))
        
        # Enviar respuesta final
        await self._send_message(WebSocketMessage(
            type=WebSocketMessageType.QUERY_RESPONSE,
            data={
                "request_id": request.request_id,
                "chunk_type": "complete",
                "content": "Full answer here...",
                "confidence": 0.85,
                "is_final": True
            }
        ))
    
    async def _send_complete_response(self, request: QueryRequest) -> None:
        """Env√≠a respuesta completa de una vez."""
        await self._send_message(WebSocketMessage(
            type=WebSocketMessageType.QUERY_RESPONSE,
            data={
                "request_id": request.request_id,
                "chunk_type": "complete",
                "content": "Complete answer here...",
                "confidence": 0.9,
                "sources": [],
                "is_final": True
            }
        ))
    
    async def _handle_ping(self, message: WebSocketMessage) -> None:
        """Maneja ping (heartbeat)."""
        self.last_ping = datetime.now()
        
        await self._send_message(WebSocketMessage(
            type=WebSocketMessageType.PONG,
            data={"timestamp": datetime.now().isoformat()}
        ))
    
    async def _handle_disconnect(self, message: WebSocketMessage) -> None:
        """Maneja desconexi√≥n."""
        self.connected = False
        await self.websocket.close()
    
    async def send_analysis_progress(self, progress: AnalysisProgressUpdate) -> None:
        """Env√≠a actualizaci√≥n de progreso de an√°lisis."""
        if "analysis_progress" not in self.subscriptions:
            return
        
        await self._send_message(WebSocketMessage(
            type=WebSocketMessageType.ANALYSIS_PROGRESS,
            data=progress.dict()
        ))
    
    async def send_change_notification(self, change: ChangeDetectionNotification) -> None:
        """Env√≠a notificaci√≥n de cambio."""
        if "change_detected" not in self.subscriptions:
            return
        
        await self._send_message(WebSocketMessage(
            type=WebSocketMessageType.CHANGE_DETECTED,
            data=change.dict()
        ))
    
    async def _send_message(self, message: WebSocketMessage) -> None:
        """Env√≠a mensaje a trav√©s del WebSocket."""
        try:
            await self.websocket.send_text(message.to_json())
        except Exception as e:
            # Log error but don't crash
            print(f"Failed to send WebSocket message: {e}")
    
    async def _send_error(self, error_message: str) -> None:
        """Env√≠a mensaje de error."""
        await self._send_message(WebSocketMessage(
            type=WebSocketMessageType.ERROR,
            data={"message": error_message}
        ))
    
    async def _heartbeat_task(self) -> None:
        """Tarea de heartbeat para mantener conexi√≥n activa."""
        while self.connected:
            try:
                # Enviar ping cada 30 segundos
                await asyncio.sleep(30)
                
                # Verificar si la conexi√≥n sigue activa
                if (datetime.now() - self.last_ping).total_seconds() > 90:
                    # Sin respuesta por 90 segundos, desconectar
                    self.connected = False
                    break
                
                # Enviar ping
                await self._send_message(WebSocketMessage(
                    type=WebSocketMessageType.PING,
                    data={"timestamp": datetime.now().isoformat()}
                ))
                
            except Exception as e:
                print(f"Heartbeat error: {e}")
                break
8. CONFIGURACI√ìN Y DESPLIEGUE COMPLETOS

8.1 Configuraci√≥n del Sistema (config/system.yaml)

yaml
# ============================================
# CONFIGURACI√ìN DEL SISTEMA - PROJECT BRAIN
# ============================================

# Entorno del sistema
system:
  name: "Project Brain"
  version: "1.0.0"
  environment: "development"  # development, staging, production
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  debug_mode: false
  data_directory: "./data"
  log_directory: "./logs"
  
  # L√≠mites del sistema
  limits:
    max_project_size_mb: 1024
    max_file_size_mb: 10
    max_concurrent_analyses: 5
    max_concurrent_queries: 100
    max_analysis_time_minutes: 30
    max_query_time_seconds: 30
    
  # Monitoreo
  monitoring:
    enabled: true
    metrics_port: 9090
    health_check_interval: 60
    performance_logging: true
    error_tracking: true

# Configuraci√≥n de proyectos
projects:
  # Extensiones soportadas
  supported_extensions:
    python: [".py", ".pyx", ".pyi"]
    javascript: [".js", ".jsx"]
    typescript: [".ts", ".tsx"]
    java: [".java"]
    cpp: [".cpp", ".c", ".h", ".hpp"]
    go: [".go"]
    rust: [".rs"]
    csharp: [".cs"]
    php: [".php"]
    ruby: [".rb"]
    
  # Patrones de exclusi√≥n
  exclude_patterns:
    - "**/node_modules/**"
    - "**/.git/**"
    - "**/__pycache__/**"
    - "**/*.min.js"
    - "**/*.min.css"
    - "**/*.log"
    - "**/test_*"
    - "**/*.spec.*"
    - "**/*.test.*"
    
  # Niveles de an√°lisis
  analysis_levels:
    quick:
      description: "An√°lisis r√°pido, solo estructura b√°sica"
      timeout_minutes: 5
      include_tests: false
      include_docs: false
      
    standard:
      description: "An√°lisis est√°ndar, estructura y m√©tricas b√°sicas"
      timeout_minutes: 15
      include_tests: true
      include_docs: true
      
    comprehensive:
      description: "An√°lisis comprehensivo, incluye dependencias y patrones"
      timeout_minutes: 30
      include_tests: true
      include_docs: true
      
    deep:
      description: "An√°lisis profundo, incluye an√°lisis de seguridad y calidad"
      timeout_minutes: 60
      include_tests: true
      include_docs: true

# Configuraci√≥n de bases de datos
databases:
  # PostgreSQL para datos relacionales
  postgresql:
    enabled: true
    host: "localhost"
    port: 5432
    database: "project_brain"
    username: "brain_user"
    password: "${DB_PASSWORD}"  # Desde variables de entorno
    pool_size: 20
    max_overflow: 10
    echo: false
    
  # Neo4j para grafo de conocimiento
  neo4j:
    enabled: true
    uri: "bolt://localhost:7687"
    username: "neo4j"
    password: "${NEO4J_PASSWORD}"
    database: "project_brain"
    max_connection_lifetime: 3600
    max_connection_pool_size: 50
    
  # Redis para cach√©
  redis:
    enabled: true
    host: "localhost"
    port: 6379
    password: "${REDIS_PASSWORD}"
    db: 0
    max_connections: 100
    socket_timeout: 5
    
  # ChromaDB para embeddings
  chromadb:
    enabled: true
    persist_directory: "./data/embeddings"
    collection_name: "project_knowledge"
    similarity_metric: "cosine"
    normalize_embeddings: true

# Configuraci√≥n de embeddings
embeddings:
  # Modelos disponibles
  models:
    text:
      default: "all-MiniLM-L6-v2"
      alternatives:
        - "all-mpnet-base-v2"
        - "multi-qa-mpnet-base-dot-v1"
        
    code:
      default: "microsoft/codebert-base"
      alternatives:
        - "microsoft/graphcodebert-base"
        - "codeparrot/codeparrot"
        
    multilingual:
      default: "intfloat/multilingual-e5-large"
      
  # Configuraci√≥n de generaci√≥n
  generation:
    device: "cpu"  # cpu, cuda, auto
    batch_size: 32
    normalize: true
    cache_embeddings: true
    cache_size: 10000
    
  # Fine-tuning
  fine_tuning:
    enabled: false
    dataset_path: "./data/fine_tuning"
    epochs: 10
    batch_size: 16
    learning_rate: 2e-5

# Configuraci√≥n de agentes
agents:
  # Agentes habilitados
  enabled:
    - "code_analyzer"
    - "qa_agent"
    - "architect"
    - "detective"
    - "curator"
    - "analyst"
    - "security_auditor"
    
  # Configuraci√≥n por agente
  code_analyzer:
    confidence_threshold: 0.7
    max_processing_time: 30
    capabilities:
      - "code_analysis"
      - "pattern_detection"
      - "quality_assessment"
      
  qa_agent:
    confidence_threshold: 0.8
    max_processing_time: 10
    capabilities:
      - "question_answering"
      - "explanation_generation"
      - "context_management"
      
  architect:
    confidence_threshold: 0.75
    max_processing_time: 60
    capabilities:
      - "architecture_review"
      - "design_pattern_detection"
      - "dependency_analysis"
      
  # Colaboraci√≥n entre agentes
  collaboration:
    enabled: true
    max_collaboration_depth: 3
    consensus_threshold: 0.6
    
  # Aprendizaje
  learning:
    enabled: true
    reinforcement_factor: 0.1
    forgetting_factor: 0.01
    feedback_integration: true

# Configuraci√≥n de APIs
api:
  # API REST
  rest:
    enabled: true
    host: "0.0.0.0"
    port: 8000
    workers: 4
    cors_origins:
      - "http://localhost:3000"
      - "http://localhost:8080"
      - "https://app.projectbrain.dev"
      
  # WebSocket
  websocket:
    enabled: true
    host: "0.0.0.0"
    port: 8001
    ping_interval: 30
    max_connections: 1000
    
  # gRPC (alta performance)
  grpc:
    enabled: false
    host: "0.0.0.0"
    port: 50051
    max_workers: 10
    
  # CLI
  cli:
    enabled: true
    
  # Autenticaci√≥n
  authentication:
    enabled: false  # Habilitar para producci√≥n
    method: "jwt"  # jwt, api_key, oauth2
    jwt_secret: "${JWT_SECRET}"
    token_expiry_hours: 24
    
  # Rate limiting
  rate_limiting:
    enabled: true
    requests_per_minute: 60
    burst_limit: 10
    
  # Documentaci√≥n
  documentation:
    enabled: true
    path: "/docs"
    openapi_url: "/openapi.json"

# Configuraci√≥n de aprendizaje
learning:
  incremental_learning: true
  feedback_integration: true
  
  # Refuerzo de conocimiento
  reinforcement:
    factor: 0.1
    decay_rate: 0.01
    min_confidence: 0.1
    
  # Olvido de conocimiento irrelevante
  forgetting:
    enabled: true
    age_threshold_days: 90
    relevance_threshold: 0.3
    strategy: "gradual"  # gradual, threshold, hybrid
    
  # Adaptaci√≥n a nuevos dominios
  adaptation:
    enabled: true
    learning_rate: 0.3
    max_domain_shift: 0.5

# Configuraci√≥n de cach√©
cache:
  # Jerarqu√≠a de cach√©
  hierarchy:
    level1:
      type: "memory"
      max_size: 1000
      ttl_seconds: 300
      
    level2:
      type: "redis"
      max_size: 10000
      ttl_seconds: 3600
      
    level3:
      type: "disk"
      max_size: 100000
      ttl_seconds: 86400
      
  # Estrategias
  strategies:
    default: "LRU"
    available:
      - "LRU"  # Least Recently Used
      - "LFU"  # Least Frequently Used
      - "TTL"  # Time To Live
      - "ARC"  # Adaptive Replacement Cache
      
  # Invalidaci√≥n
  invalidation:
    on_change: true
    on_analysis_update: true
    pattern_based: true

# Configuraci√≥n de seguridad
security:
  # Autenticaci√≥n
  authentication:
    required: false  # true en producci√≥n
    methods:
      - "api_key"
      - "jwt"
      - "oauth2"
      
    api_key:
      header: "X-API-Key"
      rotation_days: 90
      
    jwt:
      algorithm: "HS256"
      secret: "${JWT_SECRET}"
      expiry_hours: 24
      
  # Autorizaci√≥n
  authorization:
    enabled: false
    model: "RBAC"  # Role-Based Access Control
    roles:
      admin:
        permissions: ["*"]
        
      developer:
        permissions:
          - "projects:read"
          - "projects:write"
          - "analysis:start"
          - "query:ask"
          
      viewer:
        permissions:
          - "projects:read"
          - "query:ask"
          
  # Protecci√≥n de datos
  data_protection:
    encryption_at_rest: true
    encryption_in_transit: true
    key_rotation_days: 90
    
  # Sandboxing para an√°lisis
  sandbox:
    enabled: true
    level: "process"  # process, container, vm
    resource_limits:
      cpu: "2"
      memory: "4GB"
      disk: "10GB"
    network_access: false
    
  # Escaneo de seguridad
  scanning:
    pre_analysis: true
    dependency_checking: true
    malware_detection: true

# Configuraci√≥n de monitoreo
monitoring:
  # M√©tricas
  metrics:
    enabled: true
    port: 9090
    path: "/metrics"
    
  # Logging
  logging:
    enabled: true
    level: "INFO"
    format: "json"
    file: "./logs/project_brain.log"
    rotation: "daily"
    retention_days: 30
    
  # Alertas
  alerts:
    enabled: true
    channels:
      - "log"
      - "email"
      - "slack"
      
    rules:
      - name: "high_error_rate"
        condition: "error_rate > 5%"
        severity: "warning"
        
      - name: "system_down"
        condition: "health_status != 'healthy' for 5m"
        severity: "critical"
        
      - name: "high_resource_usage"
        condition: "cpu > 80% or memory > 90%"
        severity: "warning"
        
  # Dashboards
  dashboards:
    enabled: true
    grafana:
      enabled: true
      url: "http://localhost:3000"
    builtin:
      enabled: true
      port: 8080

# Configuraci√≥n de backup
backup:
  enabled: true
  schedule: "0 2 * * *"  # Diario a las 2 AM
  retention_days: 30
  location: "./data/backups"
  
  # Qu√© respaldar
  include:
    - "knowledge_graph"
    - "embeddings"
    - "configurations"
    - "logs"
    
  # Excluir
  exclude:
    - "cache"
    - "temp_files"
    
  # Verificaci√≥n
  verification:
    enabled: true
    after_backup: true
    weekly_full_verification: true

# Configuraci√≥n de despliegue
deployment:
  # Docker
  docker:
    base_image: "python:3.10-slim"
    workdir: "/app"
    user: "appuser"
    
  # Kubernetes
  kubernetes:
    namespace: "project-brain"
    replicas: 3
    resources:
      requests:
        cpu: "1000m"
        memory: "2Gi"
      limits:
        cpu: "2000m"
        memory: "4Gi"
        
    autoscaling:
      enabled: true
      min_replicas: 2
      max_replicas: 10
      target_cpu_utilization: 70
      
  # Health checks
  health_checks:
    liveness:
      path: "/health"
      initial_delay: 30
      period: 10
      
    readiness:
      path: "/ready"
      initial_delay: 5
      period: 5

# Configuraci√≥n de desarrollo
development:
  hot_reload: true
  debug_toolbar: true
  profiling: false
  test_mode: false
8.2 Docker Compose para Desarrollo

yaml
version: '3.8'

services:
  # PostgreSQL
  postgres:
    image: postgres:15
    container_name: project_brain_postgres
    environment:
      POSTGRES_DB: project_brain
      POSTGRES_USER: brain_user
      POSTGRES_PASSWORD: brain_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    networks:
      - brain_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U brain_user"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Neo4j
  neo4j:
    image: neo4j:5-community
    container_name: project_brain_neo4j
    environment:
      NEO4J_AUTH: neo4j/brain_password
      NEO4J_PLUGINS: '["apoc"]'
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_imports:/var/lib/neo4j/import
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    networks:
      - brain_network
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "brain_password", "RETURN 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis
  redis:
    image: redis:7-alpine
    container_name: project_brain_redis
    command: redis-server --requirepass brain_password --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    networks:
      - brain_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Project Brain API
  api:
    build:
      context: .
      dockerfile: Dockerfile.dev
    container_name: project_brain_api
    environment:
      # Database connections
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: project_brain
      POSTGRES_USER: brain_user
      POSTGRES_PASSWORD: brain_password
      
      NEO4J_URI: bolt://neo4j:7687
      NEO4J_USER: neo4j
      NEO4J_PASSWORD: brain_password
      
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: brain_password
      
      # System
      ENVIRONMENT: development
      LOG_LEVEL: DEBUG
      DEBUG: "true"
      
      # Paths
      DATA_DIR: /app/data
      LOG_DIR: /app/logs
      
    volumes:
      - .:/app
      - ./data:/app/data
      - ./logs:/app/logs
      - ./config:/app/config
      
    ports:
      - "8000:8000"  # REST API
      - "8001:8001"  # WebSocket
      
    depends_on:
      postgres:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      redis:
        condition: service_healthy
        
    networks:
      - brain_network
      
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      
    command: >
      sh -c "
        # Esperar que las bases de datos est√©n listas
        sleep 10 &&
        
        # Inicializar sistema si es necesario
        python scripts/init_system.py &&
        
        # Iniciar servidor con autoreload
        uvicorn src.api.server:app --host 0.0.0.0 --port 8000 --reload
      "

  # Prometheus para m√©tricas
  prometheus:
    image: prom/prometheus:latest
    container_name: project_brain_prometheus
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - brain_network
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/console_templates'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'

  # Grafana para dashboards
  grafana:
    image: grafana/grafana:latest
    container_name: project_brain_grafana
    environment:
      GF_SECURITY_ADMIN_PASSWORD: brain_password
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    ports:
      - "3000:3000"
    networks:
      - brain_network
    depends_on:
      - prometheus

  # pgAdmin (opcional, para gesti√≥n de PostgreSQL)
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: project_brain_pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@projectbrain.dev
      PGADMIN_DEFAULT_PASSWORD: brain_password
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    ports:
      - "5050:80"
    networks:
      - brain_network

networks:
  brain_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  postgres_data:
  neo4j_data:
  neo4j_logs:
  neo4j_imports:
  redis_data:
  prometheus_data:
  grafana_data:
  pgadmin_data:
8.3 Dockerfile para Producci√≥n

dockerfile
# ============================================
# DOCKERFILE PARA PRODUCCI√ìN - PROJECT BRAIN
# ============================================

# Etapa 1: Builder
FROM python:3.10-slim as builder

WORKDIR /app

# Instalar dependencias del sistema
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    build-essential \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Crear usuario no-root
RUN useradd -m -u 1000 appuser && \
    chown -R appuser:appuser /app

# Copiar requirements
COPY requirements/prod.txt .
COPY requirements/base.txt .
COPY requirements/ml.txt .
COPY requirements/databases.txt .

# Instalar dependencias Python
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r prod.txt

# Etapa 2: Runtime
FROM python:3.10-slim as runtime

WORKDIR /app

# Instalar dependencias runtime del sistema
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Crear usuario no-root
RUN useradd -m -u 1000 appuser && \
    chown -R appuser:appuser /app

# Copiar desde builder
COPY --from=builder /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# Copiar c√≥digo de la aplicaci√≥n
COPY --chown=appuser:appuser . .

# Crear directorios necesarios
RUN mkdir -p /app/data /app/logs /app/config && \
    chown -R appuser:appuser /app/data /app/logs /app/config

# Variables de entorno
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1
ENV PATH="/home/appuser/.local/bin:${PATH}"

# Cambiar a usuario no-root
USER appuser

# Exponer puertos
EXPOSE 8000  # REST API
EXPOSE 8001  # WebSocket
EXPOSE 9090  # M√©tricas

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Comando de inicio
CMD ["sh", "-c", \
    "python scripts/init_system.py && \
     uvicorn src.api.server:app --host 0.0.0.0 --port 8000 --workers 4"]
9. PLAN DE IMPLEMENTACI√ìN POR FASES

Fase 1: N√∫cleo y Infraestructura (Semanas 1-4)

Objetivo: Sistema b√°sico funcional sin agentes avanzados.

Semana	M√≥dulos	Entregables	Criterios de Aceptaci√≥n
1	Configuraci√≥n + Utils	config/, .env, logging, file utils	Sistema se inicia, logs funcionan, configuraci√≥n cargada
2	Indexer b√°sico	file_indexer.py, project_scanner.py	Escanea proyecto de ejemplo (100 archivos) en < 30s
3	Parser b√°sico	multi_language_parser.py (Python only)	Parsea archivos Python, extrae funciones/clases b√°sicas
4	API simple + CLI	server.py, endpoints b√°sicos, cli_interface.py	API responde a /health, /analyze, CLI ejecuta comandos b√°sicos
M√©tricas Fase 1:

An√°lisis de proyecto de 100 archivos Python: < 2 minutos
API response time: < 100ms para endpoints simples
Cobertura de pruebas: > 70% para m√≥dulos core
Documentaci√≥n: README completo + gu√≠a de inicio r√°pido
Fase 2: Memoria y Almacenamiento (Semanas 5-8)

Objetivo: Sistema con almacenamiento persistente funcional.

Semana	M√≥dulos	Entregables	Criterios de Aceptaci√≥n
5	Base de datos PostgreSQL	Esquemas completos, conexiones	Almacena/recupera proyectos, archivos, entidades
6	Vector Store (ChromaDB)	vector_store.py, embeddings b√°sicos	Almacena/recupera embeddings, b√∫squeda por similitud
7	Graph Database (Neo4j)	knowledge_graph.py, relaciones b√°sicas	Construye/consulta grafo de conocimiento
8	Sistema de cach√©	cache_manager.py, estrategias multi-nivel	Hit rate > 80% para consultas frecuentes
M√©tricas Fase 2:

Tiempo de b√∫squeda vectorial: < 50ms para 10K embeddings
Consultas de grafo complejas: < 100ms
Persistencia de datos: 100% de entidades sobreviven reinicio
Escalabilidad: Soporta 10K entidades por proyecto
Fase 3: An√°lisis Profundo (Semanas 9-12)

Objetivo: Sistema con an√°lisis de c√≥digo avanzado.

Semana	M√≥dulos	Entregables	Criterios de Aceptaci√≥n
9	Parsers multi-lenguaje	Soporte para 5+ lenguajes	Parsea Python, JS, Java, C++, Go con > 95% precisi√≥n
10	An√°lisis de dependencias	dependency_mapper.py, detecci√≥n de ciclos	Detecta dependencias con > 90% precisi√≥n
11	M√©tricas de calidad	quality_analyzer.py, m√©tricas est√°ndar	Calcula complejidad, mantenibilidad, cobertura
12	Detecci√≥n de patrones	pattern_detector.py, patrones comunes	Detecta 10+ patrones de dise√±o con > 85% precisi√≥n
M√©tricas Fase 3:

Precisi√≥n de an√°lisis: > 90% para Python, > 85% para otros lenguajes
Complejidad de McCabe: c√°lculo correcto para funciones complejas
Detecci√≥n de code smells: > 80% de recall
Tiempo de an√°lisis: < 10 minutos para proyecto de 1K archivos
Fase 4: Agentes B√°sicos (Semanas 13-16)

Objetivo: Sistema con agentes especializados b√°sicos.

Semana	M√≥dulos	Entregables	Criterios de Aceptaci√≥n
13	Framework de agentes	base_agent.py, agent_factory.py	Crea/ejecuta agentes, manejo de estado
14	CodeAnalyzerAgent	code_analyzer_agent.py	Analiza c√≥digo, detecta issues, sugiere mejoras
15	QuestionAnsweringAgent	qa_agent.py b√°sico	Responde preguntas simples sobre c√≥digo
16	Integraci√≥n inicial	brain_orchestrator.py funcional	Coordina agentes, maneja flujos completos
M√©tricas Fase 4:

Precisi√≥n de respuestas: > 80% para preguntas simples
Tiempo de respuesta agente: < 5 segundos
Colaboraci√≥n entre agentes: Intercambio de contexto funcional
Aprendizaje b√°sico: Mejora con feedback de usuario
Fase 5: Agentes Avanzados (Semanas 17-20)

Objetivo: Sistema con todos los agentes especializados.

Semana	M√≥dulos	Entregables	Criterios de Aceptaci√≥n
17	ArchitectAgent	architect_agent.py	Analiza arquitectura, sugiere mejoras
18	DetectiveAgent	detective_agent.py	Investiga problemas, root cause analysis
19	SecurityAgent + Curator	security_agent.py, curator_agent.py	An√°lisis de seguridad, gesti√≥n de conocimiento
20	Colaboraci√≥n avanzada	agent_orchestrator.py completo	Agentes colaboran en tareas complejas
M√©tricas Fase 5:

Precisi√≥n arquitectural: > 85% en identificaci√≥n de patrones
Detecci√≥n de vulnerabilidades: > 90% de issues cr√≠ticos
Tiempo de investigaci√≥n: < 30 segundos para problemas complejos
Efectividad colaborativa: +30% vs agentes individuales
Fase 6: Aprendizaje y Adaptaci√≥n (Semanas 21-24)

Objetivo: Sistema que aprende y mejora continuamente.

Semana	M√≥dulos	Entregables	Criterios de Aceptaci√≥n
21	Aprendizaje incremental	incremental_learner.py	Mejora conocimiento con cada interacci√≥n
22	Feedback loop	feedback_loop.py	Procesa feedback de usuario, ajusta confianza
23	Adaptaci√≥n a dominio	adaptation_engine.py	Se adapta a estilo/proyecto espec√≠fico
24	Fine-tuning modelos	Fine-tuning de embeddings	Mejora precisi√≥n para proyectos espec√≠ficos
M√©tricas Fase 6:

Mejora mensual en precisi√≥n: > 5%
Retenci√≥n de conocimiento: 100% de conocimiento relevante
Adaptaci√≥n: < 10 interacciones para ajustarse a nuevo proyecto
ROI de aprendizaje: Mejora > 2x en precisi√≥n con 100 interacciones
Fase 7: Producci√≥n y Escalabilidad (Semanas 25-28)

Objetivo: Sistema listo para producci√≥n a escala.

Semana	Tareas	Entregables	Criterios de Aceptaci√≥n
25	Optimizaci√≥n performance	Caching avanzado, √≠ndices	Latencia p95 < 500ms, throughput > 100 req/seg
26	Escalabilidad horizontal	Load balancing, sharding	Soporta 100+ proyectos concurrentes
27	Monitoreo producci√≥n	Dashboards, alertas, logs	99.9% uptime, detecci√≥n de issues < 1 minuto
28	Security hardening	Autenticaci√≥n, autorizaci√≥n, sandboxing	Pasa auditor√≠a de seguridad b√°sica
M√©tricas Fase 7:

Disponibilidad: > 99.5% uptime
Escalabilidad: 10x aumento de carga sin degradaci√≥n > 20%
Seguridad: 0 vulnerabilidades cr√≠ticas
Mantenibilidad: Deploy en < 10 minutos, rollback en < 5 minutos
Fase 8: Integraci√≥n y Ecosystem (Semanas 29-32)

Objetivo: Sistema integrado con herramientas externas.

Semana	Integraciones	Entregables	Criterios de Aceptaci√≥n
29	IDEs (VSCode, JetBrains)	Plugins, LSP	Integraci√≥n seamless con flujo de desarrollo
30	CI/CD (GitHub, GitLab)	Actions, pipelines	An√°lisis autom√°tico en PRs, reportes
31	Chat (Slack, Teams)	Bots, integraciones	Consultas desde chat, notificaciones
32	APIs externas (LLMs)	Integraci√≥n con GPT/Claude	Respuestas mejoradas con LLMs externos
M√©tricas Fase 8:

Tiempo de integraci√≥n: < 1 hora para herramientas populares
Adopci√≥n: > 80% del equipo usa diariamente
Productividad: +30% en velocidad de desarrollo
Satisfacci√≥n: CSAT > 4.5/5.0
10. AN√ÅLISIS DE POTENCIAL Y EFECTIVIDAD FINAL

10.1 Capacidades del Sistema Completado

Project Brain completamente implementado ser√° capaz de:

An√°lisis Exhaustivo:

Parsear y comprender c√≥digo en 10+ lenguajes de programaci√≥n
Analizar proyectos de hasta 1 mill√≥n de l√≠neas de c√≥digo
Detectar patrones de dise√±o, anti-patrones y code smells
Calcular m√©tricas de calidad, complejidad y mantenibilidad
Memoria Infinita:

Retener todo el conocimiento adquirido permanentemente
Mantener historial completo de cambios y evoluci√≥n
Recordar contexto de conversaciones anteriores
Aprender de cada interacci√≥n sin olvido catastr√≥fico
Respuestas Inteligentes:

Responder preguntas t√©cnicas complejas en segundos
Proporcionar explicaciones detalladas con ejemplos de c√≥digo
Sugerir mejoras espec√≠ficas y refactorizaciones
Predecir problemas antes de que ocurran
Adaptaci√≥n Continua:

Aprender del estilo de c√≥digo del equipo
Adaptarse a patrones espec√≠ficos del dominio
Mejorar precisi√≥n con cada interacci√≥n
Personalizar recomendaciones por desarrollador
Integraci√≥n Completa:

Funcionar como plugin en IDEs populares
Integrarse con pipelines de CI/CD
Proporcionar API para herramientas personalizadas
Conectarse con LLMs externos para capacidades extendidas
10.2 Impacto Esperado en Equipos de Desarrollo

Para Desarrolladores Individuales:

Reducci√≥n del 50% en tiempo de onboarding a proyectos nuevos
Disminuci√≥n del 40% en bugs introducidos por mal entendimiento
Aumento del 60% en reutilizaci√≥n de c√≥digo existente
Mejora del 70% en documentaci√≥n actualizada
Para Equipos:

Consistencia: Patrones y est√°ndares aplicados uniformemente
Conocimiento compartido: Nuevos miembros acceden a conocimiento tribal
Calidad sostenida: Detecci√≥n proactiva de degradaci√≥n de c√≥digo
Colaboraci√≥n mejorada: Contexto compartido en discusiones t√©cnicas
Para Organizaciones:

ROI positivo en 6 meses: Para equipos de >10 desarrolladores
Reducci√≥n de deuda t√©cnica: Detecci√≥n y gesti√≥n proactiva
Mejora en seguridad: Detecci√≥n temprana de vulnerabilidades
Escalabilidad: Nuevos equipos productivos m√°s r√°pido
10.3 M√©tricas de √âxito Finales

Categor√≠a	M√©trica	Objetivo	L√≠mite
Precisi√≥n	Respuestas correctas	> 90%	> 70%
An√°lisis de c√≥digo	> 95%	> 80%
Detecci√≥n de issues	> 85% recall	> 60%
Performance	Tiempo de an√°lisis	< 10 min (1K archivos)	< 30 min
Tiempo de respuesta	< 2 segundos p95	< 10 segundos
Throughput	100+ proyectos concurrentes	10 proyectos
Disponibilidad	Uptime	> 99.9%	> 99.5%
Recuperaci√≥n de errores	< 1 minuto	< 5 minutos
Usabilidad	Satisfacci√≥n (CSAT)	> 4.5/5.0	> 3.5/5.0
Adopci√≥n del equipo	> 80%	> 50%
Negocio	ROI	6 meses	12 meses
Reducci√≥n de bugs	40%	20%
Productividad	+30%	+10%
10.4 Evoluci√≥n Futura (Post-Implementaci√≥n)

Una vez implementado el sistema b√°sico, las siguientes capacidades podr√≠an a√±adirse:

Generaci√≥n de C√≥digo: Sugerir implementaciones completas basadas en especificaciones
Refactorizaci√≥n Autom√°tica: Implementar sugerencias de refactorizaci√≥n autom√°ticamente
An√°lisis Predictivo: Predecir problemas de performance/seguridad antes del deployment
Aprendizaje Federado: Aprender de m√∫ltiples organizaciones sin compartir datos sensibles
Interfaces Naturales: Voz, realidad aumentada, etc.
10.5 Conclusi√≥n

Project Brain representa un salto cu√°ntico en herramientas de desarrollo de software. Al combinar:

An√°lisis est√°tico profundo de c√≥digo multi-lenguaje
Representaci√≥n de conocimiento mediante grafos y embeddings
Agentes especializados que colaboran inteligentemente
Aprendizaje continuo que mejora con cada interacci√≥n
Memoria infinita que nunca olvida
El sistema se convierte en un cerebro colectivo para equipos de desarrollo, capaz de comprender proyectos complejos, responder preguntas t√©cnicas, sugerir mejoras y aprender continuamente.

La implementaci√≥n detallada en este documento proporciona un plan completo y ejecutable para construir este sistema, con especificaciones tan detalladas que m√∫ltiples equipos pueden trabajar en paralelo sin ambig√ºedades. Cada funci√≥n, clase, tipo de dato y flujo est√° especificado con precisi√≥n, permitiendo una implementaci√≥n coherente y de alta calidad.

Estado del Dise√±o: ‚úÖ COMPLETO, COHERENTE Y LISTO PARA IMPLEMENTACI√ìN

Pr√≥ximos Pasos:

Establecer repositorio con estructura de carpetas definida
Configurar CI/CD pipeline b√°sico
Implementar Fase 1 seg√∫n especificaciones
Ejecutar pruebas y validar criterios de aceptaci√≥n
Proceder iterativamente por fases, validando cada una antes de continuar
Este documento servir√° como fuente √∫nica de verdad para el desarrollo, garantizando que todos los componentes trabajen juntos coherentemente para crear un sistema que verdaderamente act√∫e como un "cerebro" para proyectos de software.